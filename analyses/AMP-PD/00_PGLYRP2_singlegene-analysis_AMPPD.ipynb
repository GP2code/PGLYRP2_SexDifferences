{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGLYRP2 - Single Gene Analysis in AMP-PD WGS data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PGLYRP2 from NCBI gene: hg38 chr19:15468645-15479501*\n",
    "\n",
    "- Project: Sex Differences in PGLYRP2 Variant rs892145 in Parkinson's Disease\n",
    "- Version: Python/3.10.12\n",
    "- Last Updated: 12-JUNE-2025\n",
    "- Update Description: Updated the gene coordinates (previously Ensemble which turned out to contain an upstream region) and added PC1-5 as covariates in the interaction analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "In this notebook we performed regression analyses with PGLYRP2 gene variants and PD in the WGS AMP-PD data using PLINK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Description\n",
    "\n",
    "__1. Description__\n",
    "\n",
    "-  Getting Started\n",
    "- Loading Python libraries\n",
    "- Defining functions\n",
    "- Set Paths\n",
    "- Make working directory\n",
    "\n",
    "__2. Installing packages__\n",
    "\n",
    "__3. Copy over data__\n",
    "\n",
    "__4. Create a covariate file with AMP-PD data__\n",
    "\n",
    "__5. Remove related individuals and PCA__\n",
    "\n",
    "__6. Annotation of the gene__\n",
    "\n",
    "__7. Case/Control Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the os package to interact with the environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Bring in Pandas for Dataframe functionality\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# Bring some visualization functionality \n",
    "import seaborn as sns  \n",
    "\n",
    "# numpy for basics\n",
    "import numpy as np\n",
    "\n",
    "# Use StringIO for working with file contents\n",
    "from io import StringIO\n",
    "\n",
    "# Enable IPython to display matplotlib graphs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable interaction with the FireCloud API\n",
    "from firecloud import api as fapi\n",
    "\n",
    "# Import the iPython HTML rendering for displaying links to Google Cloud Console\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Import urllib modules for building URLs to Google Cloud Console\n",
    "import urllib.parse\n",
    "\n",
    "# BigQuery for querying data\n",
    "from google.cloud import bigquery\n",
    "\n",
    "#Import Sys\n",
    "import sys as sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility routine for printing a shell command before executing it\n",
    "def shell_do(command):\n",
    "    print(f'Executing: {command}', file=sys.stderr)\n",
    "    !$command\n",
    "    \n",
    "def shell_return(command):\n",
    "    print(f'Executing: {command}', file=sys.stderr)\n",
    "    output = !$command\n",
    "    return '\\n'.join(output)\n",
    "\n",
    "# Utility routine for printing a query before executing it\n",
    "def bq_query(query):\n",
    "    print(f'Executing: {query}', file=sys.stderr)\n",
    "    return pd.read_gbq(query, project_id=BILLING_PROJECT_ID, dialect='standard')\n",
    "\n",
    "# Utility routine for display a message and a link\n",
    "def display_html_link(description, link_text, url):\n",
    "    html = f'''\n",
    "    <p>\n",
    "    </p>\n",
    "    <p>\n",
    "    {description}\n",
    "    <a target=_blank href=\"{url}\">{link_text}</a>.\n",
    "    </p>\n",
    "    '''\n",
    "\n",
    "    display(HTML(html))\n",
    "\n",
    "# Utility routines for reading files from Google Cloud Storage\n",
    "def gcs_read_file(path):\n",
    "    \"\"\"Return the contents of a file in GCS\"\"\"\n",
    "    contents = !gsutil -u {BILLING_PROJECT_ID} cat {path}\n",
    "    return '\\n'.join(contents)\n",
    "    \n",
    "def gcs_read_csv(path, sep=None):\n",
    "    \"\"\"Return a DataFrame from the contents of a delimited file in GCS\"\"\"\n",
    "    return pd.read_csv(StringIO(gcs_read_file(path)), sep=sep, engine='python')\n",
    "\n",
    "# Utility routine for displaying a message and link to Cloud Console\n",
    "def link_to_cloud_console_gcs(description, link_text, gcs_path):\n",
    "    url = '{}?{}'.format(\n",
    "        os.path.join('https://console.cloud.google.com/storage/browser',\n",
    "                     gcs_path.replace(\"gs://\",\"\")),\n",
    "        urllib.parse.urlencode({'userProject': BILLING_PROJECT_ID}))\n",
    "\n",
    "    display_html_link(description, link_text, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up billing project and data path variables\n",
    "BILLING_PROJECT_ID = os.environ['GOOGLE_PROJECT']\n",
    "WORKSPACE_NAMESPACE = os.environ['WORKSPACE_NAMESPACE']\n",
    "WORKSPACE_NAME = os.environ['WORKSPACE_NAME']\n",
    "WORKSPACE_BUCKET = os.environ['WORKSPACE_BUCKET']\n",
    "WORKSPACE_ATTRIBUTES = fapi.get_workspace(WORKSPACE_NAMESPACE, WORKSPACE_NAME).json().get('workspace',{}).get('attributes',{})\n",
    "\n",
    "## Print the information to check we are in the proper release and billing \n",
    "## This will be different for you, the user, depending on the billing project your workspace is on\n",
    "print('Billing and Workspace')\n",
    "print(f'Workspace Name @ `WORKSPACE_NAME`: {WORKSPACE_NAME}')\n",
    "print(f'Billing Project @ `BILLING_PROJECT_ID`: {BILLING_PROJECT_ID}')\n",
    "print(f'Workspace Bucket, where you can upload and download data @ `WORKSPACE_BUCKET`: {WORKSPACE_BUCKET}')\n",
    "print('')\n",
    "\n",
    "## AMP-PD v3.0\n",
    "## Explicitly define release v3.0 path as:\n",
    "# AMP_RELEASE_PATH \n",
    "# Also set paths to:\n",
    "# AMP_CLINICAL_RELEASE_PATH\n",
    "# AMP_RELEASE_GATK_PATH\n",
    "# AMP_WGS_RELEASE_PATH\n",
    "# AMP_WGS_RELEASE_PLINK_PATH\n",
    "# AMP_WGS_RELEASE_PLINK_PFILES\n",
    "\n",
    "print('AMP-PD v3.0')\n",
    "print(f'Path to AMP-PD v3.0 Clinical Data: {AMP_CLINICAL_RELEASE_PATH}')\n",
    "print(f'Path to AMP-PD v3.0 WGS Data: {AMP_WGS_RELEASE_PLINK_PATH}')\n",
    "print(f'Path to AMP-PD v3.0 WGS Data: {AMP_WGS_RELEASE_PLINK_PFILES}')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make a directory called \"/home/jupyter/PGLYRP2_AMPPD/\"\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "shell_do(f'mkdir -p {WORK_DIR}') # f' means f-string - contains expressions to execute the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/PGLYRP2_AMPPD/\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install plink 1.9\n",
    "cd /home/jupyter/\n",
    "if test -e /home/jupyter/plink; then\n",
    "    echo \"Plink is already installed in /home/jupyter/\"\n",
    "else\n",
    "    echo \"Plink is not installed\"\n",
    "    cd /home/jupyter\n",
    "\n",
    "    wget http://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20190304.zip \n",
    "\n",
    "    unzip -o plink_linux_x86_64_20190304.zip\n",
    "    mv plink plink1.9\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# chmod plink 1.9 to make sure you have permission to run the program\n",
    "chmod u+x /home/jupyter/plink1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install plink 2.0\n",
    "cd /home/jupyter/\n",
    "if test -e /home/jupyter/plink2; then\n",
    "\n",
    "echo \"Plink2 is already installed in /home/jupyter/\"\n",
    "else\n",
    "echo \"Plink2 is not installed\"\n",
    "cd /home/jupyter/\n",
    "\n",
    "wget http://s3.amazonaws.com/plink2-assets/plink2_linux_x86_64_latest.zip\n",
    "\n",
    "unzip -o plink2_linux_x86_64_latest.zip\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# chmod plink 2 to make sure you have permission to run the program\n",
    "chmod u+x /home/jupyter/plink2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install ANNOVAR: We are adding the download link after registration on the annovar website\n",
    "# https://www.openbioinformatics.org/annovar/annovar_download_form.php\n",
    "\n",
    "if test -e /home/jupyter/annovar; then\n",
    "\n",
    "echo \"annovar is already installed in /home/jupyter/notebooks\"\n",
    "else\n",
    "echo \"annovar is not installed\"\n",
    "cd /home/jupyter/\n",
    "\n",
    "wget http://www.openbioinformatics.org/annovar/download/0wgxR2rIVP/annovar.latest.tar.gz\n",
    "\n",
    "tar xvfz annovar.latest.tar.gz\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install BCFtools\n",
    "\n",
    "if test -e /home/jupyter/bcftools; then\n",
    "    echo \"BCFtools is already installed in /home/jupyter/bcftools\"\n",
    "else\n",
    "    echo \"BCFtools is not installed\"\n",
    "    cd /home/jupyter/\n",
    "\n",
    "    # Download the latest version of BCFtools\n",
    "    wget https://github.com/samtools/bcftools/releases/download/1.21/bcftools-1.21.tar.bz2\n",
    "\n",
    "    # Extract the downloaded file\n",
    "    tar -xvjf bcftools-1.21.tar.bz2\n",
    "\n",
    "    # Move into the extracted directory\n",
    "    cd bcftools-1.21\n",
    "\n",
    "    # Compile and install BCFtools\n",
    "    make\n",
    "    make install\n",
    "\n",
    "    # Move the installed BCFtools to a specific directory\n",
    "    mv bcftools /home/jupyter/bcftools\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /home/jupyter/tools/annovar/\n",
    "\n",
    "perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar refGene humandb/\n",
    "perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar clinvar_20170905 humandb/\n",
    "perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar dbnsfp47a humandb/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Install RVTESTS: Option 1 (~15min)\n",
    "if [ ! -e /home/jupyter/tooles/rvtests ]; then\n",
    "    echo \"RVTESTS not found. Installing...\"\n",
    "    mkdir -p /home/jupyter/tools/rvtests\n",
    "    cd /home/jupyter/tools/rvtests\n",
    "\n",
    "    wget https://github.com/zhanxw/rvtests/releases/download/v2.1.0/rvtests_linux64.tar.gz \n",
    "\n",
    "    tar -zxvf rvtests_linux64.tar.gz\n",
    "else\n",
    "    echo \"RVTESTS is already installed.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/tools/rvtests/executable\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 777 /home/jupyter/tools/rvtests/executable/rvtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 777 /home/jupyter/tools/rvtests/executable/rvtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "/home/jupyter/tools/rvtests/executable/rvtest --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/tools/\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Over Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check directory where AMP-PD data is\n",
    "print(\"List available imputed genotype information in AMPPD\")\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} ls {AMP_WGS_RELEASE_PLINK_PFILES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  AMP-PD release 3 data\n",
    "# PGLYRP2 from NCBI gene: hg38 (chr19:15468645-15498956)\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp {AMP_WGS_RELEASE_PLINK_PFILES}/chr19.* {WORK_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy other AMP-PD files (clinical info)\n",
    "\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {AMP_RELEASE_PATH}/amp_pd_case_control.csv {WORK_DIR}')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {AMP_CLINICAL_RELEASE_PATH}/Enrollment.csv {WORK_DIR}')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {AMP_CLINICAL_RELEASE_PATH}/Demographics.csv {WORK_DIR}')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {AMP_CLINICAL_RELEASE_PATH}/PD_Medical_History.csv  {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##refFlat\n",
    "\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp {WORKSPACE_BUCKET}/refFlat.txt {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hg38.fa\n",
    "\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp {WORKSPACE_BUCKET}/hg38.fa {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WORK_DIR='/home/jupyter/PGLYRP2_AMPPD/'\n",
    "cd $WORK_DIR\n",
    "\n",
    "wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz\n",
    "gunzip -k hg38.fa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Check files\n",
    "cd /home/jupyter/PGLYRP2_AMPPD/\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a covariate file with AMP-PD data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clinical information\n",
    "pd_case_control_df = pd.read_csv(f'{WORK_DIR}/amp_pd_case_control.csv')\n",
    "\n",
    "# visualize\n",
    "#pd_case_control_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns of interest\n",
    "pd_case_control_latest_df = pd_case_control_df[['participant_id', 'diagnosis_latest', 'case_control_other_latest']].copy()\n",
    "\n",
    "# Rename Columns\n",
    "pd_case_control_latest_df.columns = ['ID', 'LATEST_DX', 'CASE_CONTROL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check case/control value counts\n",
    "print(pd_case_control_latest_df['CASE_CONTROL'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for study origin\n",
    "pd_case_control_latest_df['COHORT']= np.where(pd_case_control_latest_df.ID.str.contains(\"LB-\"), \"LBD\",\n",
    "                                    np.where(pd_case_control_latest_df.ID.str.contains(\"PP-\"), \"PPMI\",\n",
    "                                    np.where(pd_case_control_latest_df.ID.str.contains(\"PD-\"), \"PDBP\",\n",
    "                                    np.where(pd_case_control_latest_df.ID.str.contains(\"HB-\"), \"HBS\",\n",
    "                                    np.where(pd_case_control_latest_df.ID.str.contains(\"LC-\"), \"LCC\",\n",
    "                                    np.where(pd_case_control_latest_df.ID.str.contains(\"BF-\"), \"BIOFIND\",\n",
    "                                    np.where(pd_case_control_latest_df.ID.str.contains(\"SU-\"), \"SURE-PD3\",\n",
    "                                    np.where(pd_case_control_latest_df.ID.str.contains(\"SY-\"), \"STEADY-PD3\", np.nan))))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "case_con_reduced = pd_case_control_latest_df.copy()\n",
    "case_con_reduced.drop_duplicates(subset=['ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign case control for Plink\n",
    "conditions = [\n",
    "    (case_con_reduced['CASE_CONTROL'] == \"Case\"),\n",
    "    (case_con_reduced['CASE_CONTROL'] == \"Control\")]\n",
    "## Assign cases=2, controls=1, -9 Other (as used in PLINK)\n",
    "choices = [2,1]\n",
    "case_con_reduced['PHENO'] = np.select(conditions, choices, default=-9).astype(np.int64)\n",
    "\n",
    "# set indices\n",
    "case_con_reduced.reset_index(inplace=True)\n",
    "case_con_reduced.drop(columns=[\"index\"], inplace=True)\n",
    "\n",
    "# Remove initial Pheno Column\n",
    "case_con_reduced.drop(columns=['CASE_CONTROL'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phenotype counts\n",
    "print(case_con_reduced['PHENO'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Enrollment.csv\n",
    "enrollment_df = pd.read_csv(f'{WORK_DIR}/Enrollment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns of interest\n",
    "enrollment_subset_df = enrollment_df[['participant_id', 'study_arm']].copy()\n",
    "\n",
    "# Rename columns\n",
    "enrollment_subset_df.columns = ['ID', 'ENROLL_STUDY_ARM']\n",
    "enrollment_subset_df.head()\n",
    "\n",
    "# Drop duplicates\n",
    "enrollment_subset_df.drop_duplicates(subset=['ID'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographic data\n",
    "demographics_df = pd.read_csv(f'{WORK_DIR}/Demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns\n",
    "demographics_df.rename(columns = {'participant_id':'ID'}, inplace = True)\n",
    "demographics_df.rename(columns = {'age_at_baseline':'BASELINE_AGE'}, inplace = True)\n",
    "demographics_df.rename(columns = {'race':'RACE'}, inplace = True)\n",
    "demographics_df.rename(columns = {'ethnicity':'ETHNICITY'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by visit month and Drop Duplicates\n",
    "demographics_baseline_df = demographics_df \\\n",
    ".sort_values('visit_month', ascending=True) \\\n",
    ".drop_duplicates('ID').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge last diagnostic with diagnostic at enrollement\n",
    "demographics_df_casecon = demographics_df.merge(case_con_reduced, on='ID', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recode Sex Columns\n",
    "conditions = [\n",
    "     (demographics_df_casecon['sex'] == \"Male\"),\n",
    "     (demographics_df_casecon['sex'] == \"Female\")]\n",
    "\n",
    "# 1=Male; 2=Female\n",
    "choices = [1,2]\n",
    "\n",
    "# Create new column Sex\n",
    "demographics_df_casecon['SEX'] = np.select(conditions, choices, default=None).astype(np.int64)\n",
    "\n",
    "# Remove previous Sex column\n",
    "demographics_df_casecon.drop(columns=['sex'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep only columns of interest\n",
    "demographics_df_casecon_toKeep = demographics_df_casecon[['ID', 'PHENO', 'SEX', 'RACE',\n",
    "                                                          'ETHNICITY','BASELINE_AGE', 'LATEST_DX',\n",
    "                                                          'COHORT']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Pheno with demograhic data\n",
    "enrollment_pheno_df = demographics_df_casecon_toKeep.merge(enrollment_subset_df, on='ID', how='outer')\n",
    "\n",
    "# Create FID column\n",
    "enrollment_pheno_df['FID'] = enrollment_pheno_df['ID'].values\n",
    "\n",
    "# rename ID as IID \n",
    "enrollment_pheno_df.rename(columns = {'ID':'IID'}, inplace = True)\n",
    "\n",
    "# Order columns\n",
    "reorder_enrollment_pheno_df = enrollment_pheno_df[['FID', 'IID', 'PHENO',\n",
    "                                                  'SEX', 'RACE','ETHNICITY', 'BASELINE_AGE', 'LATEST_DX',\n",
    "                                                  'COHORT', 'ENROLL_STUDY_ARM']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove individuals from the genetic registry cohorts and other enrollment categories\n",
    "#We only want to keep  controls who were originally enrolled as controls, and PD cases originally enrolled as cases\n",
    "#Will exclude prodromal, SWEDD and unknown individuals also\n",
    "#Individuals enrolled as disease control but the latest diagnosis is PD will also be excluded\n",
    "\n",
    "#Keep only individuals enrolled as Healthy Control/Disease Control or PD.\n",
    "filtered_enrollment_pheno_df = reorder_enrollment_pheno_df.copy()\n",
    "filtered_enrollment_pheno_df = filtered_enrollment_pheno_df[filtered_enrollment_pheno_df['ENROLL_STUDY_ARM'].isin(['Disease Control', \n",
    "                                                                                                                   'Healthy Control', \n",
    "                                                                                                                  'PD'])]\n",
    "#Now remove individuals with PHENO of -9 (keep only individuals with PHENO of 1 or 2)\n",
    "filtered_enrollment_pheno_df = filtered_enrollment_pheno_df[filtered_enrollment_pheno_df['PHENO'].isin([1,2])]\n",
    "\n",
    "#Now remove individuals who were enrolled with the opposite diagnosis, i.e. individuals who were enrolled as controls but have a latest diagnosis of PD\n",
    "filtered_enrollment_pheno_df = filtered_enrollment_pheno_df[((filtered_enrollment_pheno_df['PHENO'] == 2) & (filtered_enrollment_pheno_df['ENROLL_STUDY_ARM'] == 'PD')) | (filtered_enrollment_pheno_df['PHENO'] == 1)]\n",
    "\n",
    "\n",
    "#Check value counts again\n",
    "filtered_enrollment_pheno_df.groupby(['PHENO', 'ENROLL_STUDY_ARM']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file - this is intermediate not final covariate file\n",
    "#This includes all ancestries\n",
    "filtered_enrollment_pheno_df.to_csv(f'{WORK_DIR}/COVS_temp.txt', index=False, sep='\\t', na_rep='NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GenoTools ancestry\n",
    "\n",
    "This is needed for the ancestry labels. Need to do this before running the PCA as the PCs should be generated just in the population/group of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {WORKSPACE_BUCKET}/AMPPD_v3_COV_wPHENOS_wGENOTOOLS.csv {WORK_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read covariate file to get ancestry groups\n",
    "ancestries = pd.read_csv(f'{WORK_DIR}/AMPPD_v3_COV_wPHENOS_wGENOTOOLS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file has a lot of the diagnosis info that we have generated ourselves earlier.\n",
    "#We just want the ancestry label (the last column)\n",
    "\n",
    "ancestries_selected = ancestries[['ID', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for ancestry of interest\n",
    "#Change this to your ancestry of interest\n",
    "ancestry_toKeep = 'EUR'\n",
    "\n",
    "ancestries_keep = ancestries_selected.copy()\n",
    "ancestries_keep = ancestries_selected[ancestries_selected['label'] == ancestry_toKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the value counts again - this should just be the ancestry you are interested in\n",
    "ancestries_keep.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge covariate file with the ancestry file\n",
    "#Inner join - meaning we only want to keep individuals in both dataframes\n",
    "#So this will keep just individuals in the selected ancestry group\n",
    "merged = pd.merge(filtered_enrollment_pheno_df, ancestries_keep, left_on = 'IID', right_on = 'ID', how = 'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write list of individuals from the ancestry group of interest to keep from the genetic dataset\n",
    "#We need to do this before generating PCs\n",
    "individuals_toKeep = merged[['FID', 'IID']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Export file\n",
    "\n",
    "#Note this still has related individuals\n",
    "#We will remove related individuals at the next step\n",
    "\n",
    "individuals_toKeep.to_csv('/home/jupyter/PGLYRP2_AMPPD/individuals_toKeep.EUR_enroll.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove related individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the workspace bucket, copy over the file with EUR related individuals to remove (just one individual from each pair)\n",
    "#Make sure to copy over the file for the correct ancestry\n",
    "WORK_DIR='/home/jupyter/PGLYRP2_AMPPD/'\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} -m cp -r {WORKSPACE_BUCKET}/toRemove_1stand2ndDegree_Relateds_EUR.txt {WORK_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove related individuals and keep only PD and controls in the correct enrollment categories\n",
    "# Used make-bed not make-pgen because there is a bug in plink2 and it doesn't write the pgen file https://groups.google.com/g/plink2-users/c/z_ZVhacpnts/m/byzJxGl5AwAJ\n",
    "# Have included max-alleles 2 because plink cannot write bim file with multialleleic variants\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "! /home/jupyter/tools/plink2 \\\n",
    "--pfile {WORK_DIR}/chr19 \\\n",
    "--keep {WORK_DIR}/individuals_toKeep.EUR_enroll.txt \\\n",
    "--remove {WORK_DIR}/toRemove_1stand2ndDegree_Relateds_EUR.txt \\\n",
    "--make-bed \\\n",
    "--max-alleles 2 \\\n",
    "--out {WORK_DIR}/chr19_nonrelated_enroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download previously created covar file from workspace bucket \n",
    "shell_do(f'gsutil -mu {BILLING_PROJECT_ID} cp -r {WORKSPACE_BUCKET}/AMPPD_EUR.COVS.txt {WORK_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mean age and SD for cases and controls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check mean age and SD for cases and controls \n",
    "\n",
    "ancestries = {'AMPPD'}\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    final_df = pd.read_csv(f'/home/jupyter/PGLYRP2_AMPPD/AMPPD_EUR.COVS.txt', sep='\\t')\n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "\n",
    "    # Check value counts of pheno\n",
    "    pheno = final_df['PHENO'].value_counts(dropna=False)\n",
    "    print(f'Pheno: {pheno}')\n",
    "    \n",
    "    mean_age = final_df.groupby('PHENO')['AGE'].agg(['mean', 'std'])\n",
    "    print(f'Mean age and SD: {mean_age}')\n",
    "\n",
    "    sex_pct = pd.crosstab(final_df['PHENO'], final_df['SEX'], normalize='index') * 100\n",
    "    print(f'Sex %: {sex_pct}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update sex and pheno info in plink genetic files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "WORK_DIR='/home/jupyter/PGLYRP2_AMPPD/'\n",
    "cd $WORK_DIR\n",
    "head chr19_nonrelated_enroll.fam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sex and phenotype in plink files, using the AMPPD_EUR.COVS.txt file that we made\n",
    "#-update-sex and column number tells plink where to look for the sex information in the file\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/tools/plink2 \\\n",
    "--bfile {WORK_DIR}/chr19_nonrelated_enroll \\\n",
    "--update-sex {WORK_DIR}/AMPPD_EUR.COVS.txt col-num=5 \\\n",
    "--pheno {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--pheno-name PHENO \\\n",
    "--make-pgen \\\n",
    "--out {WORK_DIR}/chr19_nonrelated_enroll.updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "WORK_DIR='/home/jupyter/PGLYRP2_AMPPD/'\n",
    "cd $WORK_DIR\n",
    "head chr19_nonrelated_enroll.updated.psam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "# Extract and annotate the gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract the region using PLINK\n",
    "\n",
    "- Extract *PGLYRP2* gene \n",
    "- *PGLYRP2* coordinates: chr19:15468645-15479501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update the variant IDs\n",
    "! /home/jupyter/tools/plink2 \\\n",
    "--pfile /home/jupyter/PGLYRP2_AMPPD/chr19_nonrelated_enroll.updated\\\n",
    "--fa /home/jupyter/PGLYRP2_AMPPD/hg38.fa \\\n",
    "--memory 25000 \\\n",
    "--set-all-var-ids \"chr@:#:\\$r:\\$a\" \\\n",
    "--new-id-max-allele-len 999 --sort-vars \\\n",
    "--make-pgen \\\n",
    "--out /home/jupyter/PGLYRP2_AMPPD/chr19_nonrelated_enroll.updated_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract region using plink and make pgen files\n",
    "#PGLYRP2 coordinates: chr19:15468645-15498956\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/tools/plink2 \\\n",
    "--pfile {WORK_DIR}/chr19_nonrelated_enroll.updated_formatted \\\n",
    "--chr 19 \\\n",
    "--from-bp 15468645 \\\n",
    "--to-bp 15479501 \\\n",
    "--max-alleles 2 \\\n",
    "--mac 2 \\\n",
    "--hwe 0.0001 \\\n",
    "--make-pgen \\\n",
    "--out {WORK_DIR}/PGLYRP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## extract region using plink and make bed files\n",
    "#PGLYRP2 coordinates: chr19:15468645-15498956\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "    \n",
    "! /home/jupyter/plink2 \\\n",
    "--pfile {WORK_DIR}/chr19_nonrelated_enroll.updated_formatted \\\n",
    "--chr 19 \\\n",
    "--from-bp 15468645 \\\n",
    "--to-bp 15479501 \\\n",
    "--mac 2 \\\n",
    "--max-alleles 2 \\\n",
    "--hwe 0.0001 \\\n",
    "--make-bed \\\n",
    "--out {WORK_DIR}/PGLYRP2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize bim file\n",
    "! head /home/jupyter/PGLYRP2_AMPPD/PGLYRP2.bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize fam file\n",
    "! head /home/jupyter/PGLYRP2_AMPPD/PGLYRP2.fam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn binary files into VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Turn binary files into VCF for annovar input\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/tools/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--recode vcf id-paste=iid \\\n",
    "--out {WORK_DIR}/PGLYRP2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Bgzip and Tabix (zip and index the file)\n",
    "for ancestry in ancestries:    \n",
    "    ! bgzip -f {WORK_DIR}/PGLYRP2.vcf\n",
    "    ! tabix -f -p vcf {WORK_DIR}/PGLYRP2.vcf.gz \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate using ANNOVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## annotate using ANNOVAR\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! perl /home/jupyter/tools/annovar/table_annovar.pl {WORK_DIR}/PGLYRP2.vcf.gz /home/jupyter/tools/annovar/humandb/ -buildver hg38 \\\n",
    "-out {WORK_DIR}/PGLYRP2.annovar \\\n",
    "-remove -protocol refGene,clinvar_20170905,dbnsfp47a \\\n",
    "-operation g,f,f \\\n",
    "--nopolish \\\n",
    "-nastring . \\\n",
    "-vcfinput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.annovar.hg38_multianno.txt',sep='\\t')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Read in ANNOVAR multianno file\n",
    "    gene = pd.read_csv(f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.annovar.hg38_multianno.txt', sep = '\\t')\n",
    "    \n",
    "    #Filter for the correct gene name (sometimes other genes are also included)\n",
    "    gene = gene[gene['Gene.refGene'] == 'PGLYRP2']\n",
    "    \n",
    "    # Convert the CADD scores to float, set errors to NaN\n",
    "    gene['CADD_phred'] = pd.to_numeric(gene['CADD_phred'], errors='coerce')\n",
    "\n",
    "    #Print number of variants in the different categories\n",
    "    results = [] \n",
    "\n",
    "    intronic = gene[gene['Func.refGene']== 'intronic']\n",
    "    upstream = gene[gene['Func.refGene']== 'upstream']\n",
    "    downstream = gene[gene['Func.refGene']== 'downstream']\n",
    "    utr5 = gene[gene['Func.refGene']== 'UTR5']\n",
    "    utr3 = gene[gene['Func.refGene']== 'UTR3']\n",
    "    splicing = gene[gene['Func.refGene']== 'splicing']\n",
    "    exonic = gene[gene['Func.refGene']== 'exonic']\n",
    "    stopgain = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'stopgain')]\n",
    "    stoploss = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'stoploss')]\n",
    "    startloss = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'startloss')]\n",
    "    frameshift_deletion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'frameshift deletion')]\n",
    "    frameshift_insertion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'frameshift insertion')]\n",
    "    nonframeshift_deletion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'nonframeshift deletion')]\n",
    "    nonframeshift_insertion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'nonframeshift insertion')]\n",
    "    coding_nonsynonymous = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'nonsynonymous SNV')]\n",
    "    coding_synonymous = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'synonymous SNV')]\n",
    "        \n",
    "    print('AMPPD')\n",
    "    print('Total variants: ', len(gene))\n",
    "    print(\"Intronic: \", len(intronic))\n",
    "    print(\"Upstream: \", len(upstream))\n",
    "    print(\"Downstream: \", len(downstream))\n",
    "    print('UTR3: ', len(utr3))\n",
    "    print('UTR5: ', len(utr5))\n",
    "    print(\"Splicing: \", len(splicing))\n",
    "    print(\"Total exonic: \", len(exonic))\n",
    "    print(\"Stopgain: \", len(stopgain))\n",
    "    print(\"Stoploss: \", len(stoploss))\n",
    "    print(\"Startloss: \", len(startloss))\n",
    "    print(\"Frameshift deletion: \", len(frameshift_deletion))\n",
    "    print(\"Frameshift insertion: \", len(frameshift_insertion))\n",
    "    print(\"Non-frameshift insertion: \", len(nonframeshift_insertion))\n",
    "    print(\"Non-frameshift deletion: \", len(nonframeshift_deletion))\n",
    "    print('Synonymous: ', len(coding_synonymous))\n",
    "    print(\"Nonsynonymous: \", len(coding_nonsynonymous))\n",
    "    results.append((gene, intronic, upstream, downstream, utr3, utr5, splicing,exonic,stopgain,stoploss,startloss, frameshift_deletion,frameshift_insertion,nonframeshift_deletion,nonframeshift_insertion,coding_synonymous, coding_nonsynonymous))\n",
    "    print('\\n')\n",
    "    \n",
    "    ## For rvtests\n",
    "    \n",
    "    # Potential functional: These are variants annotated as frameshift, nonframeshift, startloss, stoploss, stopgain, splicing, missense, exonic, UTR5, UTR3, upstream (-100bp), downstream (+100bp), or ncRNA. \n",
    "    potentially_functional = gene[gene['Func.refGene'] != 'intronic']\n",
    "    # Coding: These are variants annotated as frameshift, nonframeshift, startloss, stoploss, stopgain, splicing, or missense.\n",
    "    coding_variants = gene[(gene['Func.refGene'] == 'splicing') | (gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] != 'synonymous SNV')]\n",
    "    # Loss of function: These are variants annotated as frameshift, startloss,stopgain, or splicing.\n",
    "    loss_of_function = gene[(gene['Func.refGene'] == 'splicing') | (gene['ExonicFunc.refGene'] == 'stopgain') | (gene['ExonicFunc.refGene'] == 'startloss') | (gene['ExonicFunc.refGene'] == 'frameshift deletion') | (gene['ExonicFunc.refGene'] == 'frameshift insertion')]\n",
    "    \n",
    "    \n",
    "    # Save in PLINK format\n",
    "    variants_toKeep = potentially_functional[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep.to_csv(f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.potentially_functional.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    variants_toKeep2 = coding_variants[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep2.to_csv(f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.coding_variants.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    variants_toKeep3 = loss_of_function[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep3.to_csv(f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.loss_of_function.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "        \n",
    "    maf_01 = gene[gene['Otherinfo1'] < 0.01]\n",
    "    variants_toKeep4 = maf_01[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep4.to_csv(f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.maf_01.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    maf_03 = gene[gene['Otherinfo1'] < 0.03]\n",
    "    variants_toKeep5 = maf_03[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep5.to_csv(f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.maf_03.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "    \n",
    "\n",
    "    # For assoc\n",
    "    \n",
    "    # These are all exonic variants\n",
    "    exonic = gene[gene['Func.refGene'] == 'exonic']\n",
    "    \n",
    "    # Save in PLINK format\n",
    "    variants_toKeep7 = exonic[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep7.to_csv(f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.exonic.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head /home/jupyter/PGLYRP2_AMPPD/PGLYRP2.maf_03.variantstoKeep.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burden Analyses using RVTests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop over the different variant classes, extract variants and create the right format for RVTest\n",
    "variant_classes = ['potentially_functional', 'coding_variants','loss_of_function','maf_01','maf_03']\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "#Loop over all the ancestries and the 3 variant classes\n",
    "for ancestry in ancestries:\n",
    "    for variant_class in variant_classes:\n",
    "                \n",
    "        # Print the command to be executed (for debugging purposes)\n",
    "        print(f'Running plink to extract {variant_class} variants for ancestry: {ancestry}')\n",
    "        \n",
    "        #Extract relevant variants\n",
    "        ! /home/jupyter/plink2 \\\n",
    "        --pfile {WORK_DIR}/PGLYRP2 \\\n",
    "        --extract range {WORK_DIR}/PGLYRP2.{variant_class}.variantstoKeep.txt \\\n",
    "        --recode vcf-iid \\\n",
    "        --out {WORK_DIR}/PGLYRP2.{variant_class}\n",
    "        \n",
    "        # Print the command to be executed (for debugging purposes)\n",
    "        print(f'Running bgzip and tabix for {variant_class} variants for ancestry AMP-PD')\n",
    "        \n",
    "        ## Bgzip and Tabix (zip and index the file)\n",
    "        ! bgzip -f {WORK_DIR}/PGLYRP2.{variant_class}.vcf\n",
    "        ! tabix -f -p vcf {WORK_DIR}/PGLYRP2.{variant_class}.vcf.gz\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop over the different variant classes \n",
    "#Run with all covariates (including age)\n",
    "#Run RVtests\n",
    "\n",
    "variant_classes = ['potentially_functional', 'coding_variants','loss_of_function','maf_01','maf_03']\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "\n",
    "for variant_class in variant_classes:\n",
    "    \n",
    "    # Print the command to be executed (for debugging purposes)\n",
    "    print(f'Running RVtests for {variant_class} variants for ancestry AMP-PD')\n",
    "        \n",
    "    ## RVtests with covariates \n",
    "    #Make sure the pheno and covariate file starts with the first 5 columsn: fid, iid, fatid, matid, sex\n",
    "    #The pheno-name flag only works when the pheno/covar file is structured properly\n",
    "    ! /home/jupyter/tools/rvtests/executable/rvtest --noweb --hide-covar \\\n",
    "    --out {WORK_DIR}/PGLYRP2.burden.{variant_class} \\\n",
    "    --kernel skat,skato \\\n",
    "    --inVcf {WORK_DIR}/PGLYRP2.{variant_class}.vcf.gz \\\n",
    "    --pheno {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "    --pheno-name PHENO \\\n",
    "    --gene PGLYRP2 \\\n",
    "    --geneFile {WORK_DIR}/refFlat.txt \\\n",
    "    --covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "    --covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.potentially_functional.Skat.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.coding_variants.Skat.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.maf_03.Skat.assoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No loss of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.potentially_functional.SkatO.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.coding_variants.SkatO.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.maf_03.SkatO.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop over the different variant classes \n",
    "#Run without age as a covariate (this has been done for the GP2 data and we want to do the same also here)\n",
    "#Run RVtests\n",
    "\n",
    "variant_classes = ['potentially_functional', 'coding_variants','loss_of_function','maf_01','maf_03']\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "\n",
    "for variant_class in variant_classes:\n",
    "    \n",
    "    # Print the command to be executed (for debugging purposes)\n",
    "    print(f'Running RVtests for {variant_class} variants for ancestry AMP-PD')\n",
    "        \n",
    "    ## RVtests with covariates \n",
    "    #Make sure the pheno and covariate file starts with the first 5 columsn: fid, iid, fatid, matid, sex\n",
    "    #The pheno-name flag only works when the pheno/covar file is structured properly\n",
    "    ! /home/jupyter/tools/rvtests/executable/rvtest --noweb --hide-covar \\\n",
    "    --out {WORK_DIR}/PGLYRP2.burden.{variant_class}_NOAGE \\\n",
    "    --kernel skat,skato \\\n",
    "    --inVcf {WORK_DIR}/PGLYRP2.{variant_class}.vcf.gz \\\n",
    "    --pheno {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "    --pheno-name PHENO \\\n",
    "    --gene PGLYRP2 \\\n",
    "    --geneFile {WORK_DIR}/refFlat.txt \\\n",
    "    --covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "    --covar-name SEX,PC1,PC2,PC3,PC4,PC5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.potentially_functional_NOAGE.Skat.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.coding_variants_NOAGE.Skat.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.maf_03_NOAGE.Skat.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.potentially_functional_NOAGE.SkatO.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.coding_variants_NOAGE.SkatO.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! cat {WORK_DIR}/PGLYRP2.burden.maf_03_NOAGE.SkatO.assoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case/Control Analysis\n",
    "\n",
    "### Glossary\n",
    "\n",
    "- CHR Chromosome code\n",
    "- SNP Variant identifier\n",
    "- A1 Allele 1 (usually minor)\n",
    "- A2 Allele 2 (usually major)\n",
    "- MAF Allele 1 frequency in all subjects\n",
    "- F_A/MAF_A Allele 1 frequency in cases\n",
    "- F_U/MAF_U Allele 1 frequency in controls\n",
    "- NCHROBS_A Number of case allele observations\n",
    "- NCHROBS_U Number of control allele observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL VARIANTS\n",
    "#### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis using plink assoc for ALL variants, not adjusting for any covariates\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "!/home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--assoc \\\n",
    "--maf 0.01 \\\n",
    "--mac 2 \\\n",
    "--hwe 0.0001 \\\n",
    "--adjust \\\n",
    "--allow-no-sex \\\n",
    "--ci 0.95 \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A).\n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--recode A \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink assoc unadjusted analysis\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "#Look at assoc results\n",
    "freq = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants.assoc', delim_whitespace=True)\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "sig_all_nonadj = freq[freq['P']<0.05]\n",
    "    \n",
    "print(f'There are {len(sig_all_nonadj)} variants with p-value < 0.05')\n",
    "\n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "        \n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['SNP'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "          \n",
    "#Merge with the assoc file\n",
    "sig_merge = freq[['SNP','A1','F_A','F_U','A2','L95','OR','U95','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='SNP', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}') \n",
    "    \n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_assoc.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - Adjusting for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates (including age)\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--glm \\\n",
    "--adjust \\\n",
    "--maf 0.01 \\\n",
    "--mac 2 \\\n",
    "--ci 0.95 \\\n",
    "--hwe 0.0001 \\\n",
    "--covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--covar-variance-standardize \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_age\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--recode A \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants and all covariates\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "print(f'WORKING ON: AMP-PD')\n",
    "    \n",
    "#Read in glm results\n",
    "assoc = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "significant = assoc_add[assoc_add['P']<0.05]\n",
    "print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "\n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_age.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "     })\n",
    "\n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "#Merge with the glm file\n",
    "sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}')\n",
    "\n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the glm file\n",
    "\n",
    "glm_age = pd.read_csv('/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid',sep='\\t')\n",
    "glm_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    # Read file using whitespace as delimiter\n",
    "    glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "    # Sort the DataFrame by Bonferroni-corrected p-value (BONF), smallest to largest\n",
    "    sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True)\n",
    "\n",
    "    print(f\"\\nTop entries for AMP-PD\")\n",
    "    print(sorted_glm.head())  # Use .to_string(index=False) for cleaner output if desired\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing AMP-PD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - Not adjusting for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates but without age\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--glm \\\n",
    "--adjust \\\n",
    "--maf 0.01 \\\n",
    "--mac 2 \\\n",
    "--ci 0.95 \\\n",
    "--hwe 0.0001 \\\n",
    "--covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--covar-name SEX,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--covar-variance-standardize \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_noage\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--recode A \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_noage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants (without age as a covariate)\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "    \n",
    "print(f'WORKING ON: AMP-PD')\n",
    "    \n",
    "#Read in glm results\n",
    "assoc = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "significant = assoc_add[assoc_add['P']<0.05]\n",
    "print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_noage.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "    })\n",
    "\n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "#Merge with the glm file\n",
    "sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage.txt', sep = '\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Look at the glm file\n",
    "\n",
    "glm_noage = pd.read_csv('/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid',sep='\\t')\n",
    "glm_noage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    # Read file using whitespace as delimiter\n",
    "    glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "    # Sort the DataFrame by Bonferroni-corrected p-value (BONF), smallest to largest\n",
    "    sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True)\n",
    "\n",
    "    print(f\"\\nTop entries for AMP-PD\")\n",
    "    print(sorted_glm.head())  # Use .to_string(index=False) for cleaner output if desired\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing AMP-PD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting information on rs892145 for all ancestries + HWE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract information on the variant of interest (rs892145) from the association results\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_assoc.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the results from the glm analyses for the variant of interest\n",
    "\n",
    "# Variant to search for\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the adjusted p-value information from the glm analyses for the variant of interest\n",
    "\n",
    "# Target variant and file path for AMPPD\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "filename = '/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where the ID column contains the target variant\n",
    "    match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print the results from the glm analyses for the variant of interest\n",
    "\n",
    "# Variant to search for\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the adjusted p-value information from the glm analyses for the variant of interest\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "filename = '/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where the ID column contains the target variant\n",
    "    match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HWE - rs892145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## extract the SNP of interst and calculate HWE (controls only)\n",
    "\n",
    "! /home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--chr 19 \\\n",
    "--from-bp 15475861 \\\n",
    "--to-bp 15475861 \\\n",
    "--keep-if PHENO1==1 \\\n",
    "--hardy \\\n",
    "--out {WORK_DIR}/PGLYRP2_HWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the HWE file\n",
    "\n",
    "hwe = pd.read_csv('/home/jupyter/PGLYRP2_AMPPD/PGLYRP2_HWE.hardy',sep='\\t')\n",
    "hwe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex stratified analyses\n",
    "#### assoc - males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis using plink assoc for ALL variants, not adjusting for any covariates and only in males (=1)\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "!/home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--assoc \\\n",
    "--maf 0.01 \\\n",
    "--filter-males \\\n",
    "--mac 2 \\\n",
    "--hwe 0.0001 \\\n",
    "--adjust \\\n",
    "--allow-no-sex \\\n",
    "--ci 0.95 \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_males\n",
    "\n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A).\n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--filter-males \\\n",
    "--recode A \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_males\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink assoc unadjusted analysis\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "#Look at assoc results\n",
    "freq = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_males.assoc', delim_whitespace=True)\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "sig_all_nonadj = freq[freq['P']<0.05]\n",
    "    \n",
    "print(f'There are {len(sig_all_nonadj)} variants with p-value < 0.05')\n",
    "\n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_males.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "        \n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['SNP'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "          \n",
    "#Merge with the assoc file\n",
    "sig_merge = freq[['SNP','A1','F_A','F_U','A2','L95','OR','U95','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='SNP', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}') \n",
    "    \n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_assoc_males.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - males - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates (including age) only in males\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--glm \\\n",
    "--adjust \\\n",
    "--maf 0.01 \\\n",
    "--filter-males \\\n",
    "--mac 2 \\\n",
    "--ci 0.95 \\\n",
    "--hwe 0.0001 \\\n",
    "--covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--covar-name AGE,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--covar-variance-standardize \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_age_males\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--filter-males \\\n",
    "--recode A \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_age_males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants and all covariates run only in males\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "print(f'WORKING ON: AMP-PD')\n",
    "    \n",
    "#Read in glm results\n",
    "assoc = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "significant = assoc_add[assoc_add['P']<0.05]\n",
    "print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "\n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_age_males.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "     })\n",
    "\n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "#Merge with the glm file\n",
    "sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}')\n",
    "\n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age_males.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    # Read file using whitespace as delimiter\n",
    "    glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "    # Sort the DataFrame by Bonferroni-corrected p-value (BONF), smallest to largest\n",
    "    sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True)\n",
    "\n",
    "    print(f\"\\nTop entries for AMP-PD\")\n",
    "    print(sorted_glm.head())  # Use .to_string(index=False) for cleaner output if desired\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing AMP-PD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - males - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates but without age only in males\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--glm \\\n",
    "--adjust \\\n",
    "--maf 0.01 \\\n",
    "--filter-males \\\n",
    "--mac 2 \\\n",
    "--ci 0.95 \\\n",
    "--hwe 0.0001 \\\n",
    "--covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--covar-name PC1,PC2,PC3,PC4,PC5 \\\n",
    "--covar-variance-standardize \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_noage_males\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--recode A \\\n",
    "--filter-males \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_noage_males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants (without age as a covariate)\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "    \n",
    "print(f'WORKING ON: AMP-PD')\n",
    "    \n",
    "#Read in glm results\n",
    "assoc = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "significant = assoc_add[assoc_add['P']<0.05]\n",
    "print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_noage_males.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "    })\n",
    "\n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "#Merge with the glm file\n",
    "sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage_males.txt', sep = '\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    # Read file using whitespace as delimiter\n",
    "    glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "    # Sort the DataFrame by Bonferroni-corrected p-value (BONF), smallest to largest\n",
    "    sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True)\n",
    "\n",
    "    print(f\"\\nTop entries for AMP-PD\")\n",
    "    print(sorted_glm.head())  # Use .to_string(index=False) for cleaner output if desired\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing AMP-PD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assoc - females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis using plink assoc for ALL variants, not adjusting for any covariates and only in females (=2)\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "!/home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--assoc \\\n",
    "--maf 0.01 \\\n",
    "--mac 2 \\\n",
    "--hwe 0.0001 \\\n",
    "--filter-females \\\n",
    "--adjust \\\n",
    "--allow-no-sex \\\n",
    "--ci 0.95 \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_females\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A).\n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--recode A \\\n",
    "--filter-females \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink assoc unadjusted analysis in females\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "#Look at assoc results\n",
    "freq = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_females.assoc', delim_whitespace=True)\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "sig_all_nonadj = freq[freq['P']<0.05]\n",
    "    \n",
    "print(f'There are {len(sig_all_nonadj)} variants with p-value < 0.05')\n",
    "\n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_females.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "        \n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['SNP'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "          \n",
    "#Merge with the assoc file\n",
    "sig_merge = freq[['SNP','A1','F_A','F_U','A2','L95','OR','U95','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='SNP', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}') \n",
    "    \n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_assoc_females.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - females - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates in females only\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--glm \\\n",
    "--adjust \\\n",
    "--maf 0.01 \\\n",
    "--filter-females \\\n",
    "--mac 2 \\\n",
    "--ci 0.95 \\\n",
    "--hwe 0.0001 \\\n",
    "--covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--covar-name AGE,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--covar-variance-standardize \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_age_females\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--recode A \\\n",
    "--filter-females \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_age_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants and all covariates in females only\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "print(f'WORKING ON: AMP-PD')\n",
    "    \n",
    "#Read in glm results\n",
    "assoc = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "significant = assoc_add[assoc_add['P']<0.05]\n",
    "print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "\n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_age_females.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "     })\n",
    "\n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "#Merge with the glm file\n",
    "sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}')\n",
    "\n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age_females.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    # Read file using whitespace as delimiter\n",
    "    glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "    # Sort the DataFrame by Bonferroni-corrected p-value (BONF), smallest to largest\n",
    "    sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True)\n",
    "\n",
    "    print(f\"\\nTop entries for AMP-PD\")\n",
    "    print(sorted_glm.head())  # Use .to_string(index=False) for cleaner output if desired\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing AMP-PD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - females - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates in females only\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "! /home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--glm \\\n",
    "--adjust \\\n",
    "--maf 0.01 \\\n",
    "--filter-females \\\n",
    "--mac 2 \\\n",
    "--ci 0.95 \\\n",
    "--hwe 0.0001 \\\n",
    "--covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--covar-name PC1,PC2,PC3,PC4,PC5 \\\n",
    "--covar-variance-standardize \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_noage_females\n",
    "    \n",
    "#--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "! /home/jupyter/plink1.9 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--filter-females \\\n",
    "--recode A \\\n",
    "--out {WORK_DIR}/PGLYRP2.allvariants_noage_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants (without age as a covariate) in females only\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "    \n",
    "print(f'WORKING ON: AMP-PD')\n",
    "    \n",
    "#Read in glm results\n",
    "assoc = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "#Filter for significant variants p < 0.05 - if any\n",
    "significant = assoc_add[assoc_add['P']<0.05]\n",
    "print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "#Read in plink recoded data (.raw file)\n",
    "recode = pd.read_csv(f'{WORK_DIR}/PGLYRP2.allvariants_noage_females.raw', delim_whitespace=True)\n",
    "\n",
    "# Make a list from the column names\n",
    "column_names = recode.columns.tolist()\n",
    "\n",
    "# Drop the first 6 columns to keep the variants \n",
    "variants = column_names[6:]\n",
    "\n",
    "print(f'Number of variants in AMP-PD for PGLYRP2: {len(variants)}')\n",
    "\n",
    "# Pre-filter the dataset\n",
    "cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Pre-filter the dataset\n",
    "total_cases = cases_data.shape[0]\n",
    "total_controls = controls_data.shape[0]\n",
    "results = []\n",
    "\n",
    "for variant in variants:\n",
    "    ## For PD cases\n",
    "    hom_cases = (cases_data[variant] == 2).sum()\n",
    "    het_cases = (cases_data[variant] == 1).sum()\n",
    "    hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "    freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "    ## For controls\n",
    "    hom_controls = (controls_data[variant] == 2).sum()\n",
    "    het_controls = (controls_data[variant] == 1).sum()\n",
    "    hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "    missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "    freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "    # Append results in dictionary format\n",
    "    results.append({\n",
    "        'Variant': variant,\n",
    "        'Hom Cases': hom_cases,\n",
    "        'Het Cases': het_cases,\n",
    "        'Hom Ref Cases': hom_ref_cases,\n",
    "        'Missing Cases': missing_cases,\n",
    "        'Total Cases': total_cases,\n",
    "        'Carrier Freq in Cases': freq_cases,\n",
    "        'Hom Controls': hom_controls,\n",
    "        'Het Controls': het_controls,\n",
    "        'Hom Ref Controls': hom_ref_controls,\n",
    "        'Missing Controls': missing_controls,\n",
    "        'Total Controls': total_controls,\n",
    "        'Carrier Freq in Controls': freq_controls\n",
    "    })\n",
    "\n",
    "# Return\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "#Print dimensions of the df_results dataframe\n",
    "print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "#Merge with the glm file\n",
    "sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "#Print dimensions of the merged dataframe (just adding more columns)\n",
    "print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "## Save to CSV\n",
    "merged.to_csv(f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage_females.txt', sep = '\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    # Read file using whitespace as delimiter\n",
    "    glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "\n",
    "    # Sort the DataFrame by Bonferroni-corrected p-value (BONF), smallest to largest\n",
    "    sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True)\n",
    "\n",
    "    print(f\"\\nTop entries for AMP-PD\")\n",
    "    print(sorted_glm.head())  # Use .to_string(index=False) for cleaner output if desired\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing AMP-PD: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information on rs892145 only in males vs females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### males"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information on the variant of interest (rs892145) from the association results in males\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_assoc_males.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the results from the glm analyses for the variant of interest\n",
    "\n",
    "# Variant to search for\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age_males.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the adjusted p-value information from the glm analyses for the variant of interest in males only\n",
    "\n",
    "# Target variant and file path for AMPPD\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "filename = '/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where the ID column contains the target variant\n",
    "    match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the results from the glm analyses for the variant of interest (without age as covariate) only in males\n",
    "\n",
    "# Variant to search for\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage_males.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the adjusted p-value information from the glm analyses for the variant of interest\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "filename = '/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where the ID column contains the target variant\n",
    "    match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information on the variant of interest (rs892145) from the association results in males\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_assoc_females.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the results from the glm analyses for the variant of interest in females\n",
    "\n",
    "# Variant to search for\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age_females.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the adjusted p-value information from the glm analyses for the variant of interest in females only\n",
    "\n",
    "# Target variant and file path for AMPPD\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "filename = '/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where the ID column contains the target variant\n",
    "    match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the results from the glm analyses for the variant of interest (without age as covariate) only in females\n",
    "\n",
    "# Variant to search for\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "filename = f'{WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage_females.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where Variant column contains the target variant\n",
    "    match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the adjusted p-value information from the glm analyses for the variant of interest in females\n",
    "\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "filename = '/home/jupyter/PGLYRP2_AMPPD/PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "\n",
    "    # Filter the row where the ID column contains the target variant\n",
    "    match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "\n",
    "    if not match.empty:\n",
    "        print(\"AMPPD:\")\n",
    "        print(match.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"No match found in AMPPD.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install R \n",
    "\n",
    "!pip install rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rpy2 and activate the R interface\n",
    "\n",
    "import rpy2.robjects as robjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if R is working correctly \n",
    "\n",
    "robjects.r('R.version.string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use R natively in a cell, load the R magic extension\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"chr19:15475861:A:T\" > /home/jupyter/PGLYRP2_AMPPD/single_snp.txt\n",
    "!echo \"chr19:15475861:A:T T\" > /home/jupyter/PGLYRP2_AMPPD/reference_allele.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new covariate file for R where genotypes are coded additively (0,1,2)\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "!/home/jupyter/plink2 \\\n",
    "--bfile {WORK_DIR}/PGLYRP2 \\\n",
    "--extract {WORK_DIR}/single_snp.txt \\\n",
    "--pheno {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--pheno-name PHENO \\\n",
    "--covar {WORK_DIR}/AMPPD_EUR.COVS.txt \\\n",
    "--covar-name SEX,PC1,PC2,PC3,PC4,PC5 \\\n",
    "--ref-allele {WORK_DIR}/reference_allele.txt \\\n",
    "--recode A \\\n",
    "--out {WORK_DIR}/PGLYRP2_interaction\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head {WORK_DIR}/PGLYRP2_interaction.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {WORK_DIR}/PGLYRP2_interaction.cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the ANC_PGLYRP2_interaction.raw with the ANC_PGLYRP2_interaction.cov on IID\n",
    "#Drop SEX from one of the files to avoid duplicates\n",
    "\n",
    "cov_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2_interaction.cov'\n",
    "raw_path = f'/home/jupyter/PGLYRP2_AMPPD/PGLYRP2_interaction.raw'\n",
    "\n",
    "try:\n",
    "    # Load both files\n",
    "    cov_df = pd.read_csv(cov_path, delim_whitespace=True)\n",
    "    raw_df = pd.read_csv(raw_path, delim_whitespace=True)\n",
    "\n",
    "    # Rename '#IID' to 'IID' for merging\n",
    "    cov_df = cov_df.rename(columns={'#IID': 'IID'})\n",
    "\n",
    "    # Drop 'SEX' if present to avoid duplication\n",
    "    cov_df = cov_df.drop(columns=['SEX'], errors='ignore')\n",
    "\n",
    "    # Merge on 'IID'\n",
    "    merged_df = pd.merge(raw_df, cov_df, on='IID', how='inner')\n",
    "\n",
    "    # Save merged DataFrame back to raw path\n",
    "    merged_df.to_csv(raw_path, sep=' ', index=False)\n",
    "\n",
    "    print(f\"Saved merged file for AMPPD: {raw_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Missing file for AMPPD\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing AMPPD: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#prepare the file - transform pheno coding, put sex as categorical and set female as the reference group\n",
    "\n",
    "file_path <- \"/home/jupyter/PGLYRP2_AMPPD/PGLYRP2_interaction.raw\"\n",
    "\n",
    "# Check if file exists before processing\n",
    "if (file.exists(file_path)) {\n",
    "  # Read the file\n",
    "  interaction_data <- read.table(file_path, header = TRUE)\n",
    "  \n",
    "  # Transform PHENOTYPE coding\n",
    "  interaction_data$PHENOTYPE <- ifelse(interaction_data$PHENOTYPE == 2, 1, 0)\n",
    "  \n",
    "  # Transform SEX to categorical and relevel with \"Female\" as reference\n",
    "  interaction_data$SEX <- factor(interaction_data$SEX, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n",
    "  interaction_data$SEX <- relevel(interaction_data$SEX, ref = \"Female\")\n",
    "  \n",
    "  # Print a quick summary or head\n",
    "  cat(\"\\nProcessed: AMPPD\\n\")\n",
    "  print(head(interaction_data))\n",
    "} else {\n",
    "  cat(\"\\nFile not found for AMPPD\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# File path for AMPPD interaction file\n",
    "file_path <- \"/home/jupyter/PGLYRP2_AMPPD/PGLYRP2_interaction.raw\"\n",
    "\n",
    "if (file.exists(file_path)) {\n",
    "  interaction_data <- read.table(file_path, header = TRUE)\n",
    "  \n",
    "  # Transform PHENOTYPE coding\n",
    "  interaction_data$PHENOTYPE <- ifelse(interaction_data$PHENOTYPE == 2, 1, 0)\n",
    "  \n",
    "  # Transform SEX to categorical and relevel with \"Female\" as reference\n",
    "  interaction_data$SEX <- factor(interaction_data$SEX, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n",
    "  interaction_data$SEX <- relevel(interaction_data$SEX, ref = \"Female\")\n",
    "  \n",
    "  # Fit logistic regression model with interaction term\n",
    "  glm_interaction <- glm(PHENOTYPE ~ SEX * `chr19.15475861.A.T_T` + PC1+PC2+PC3+PC4+PC5, data = interaction_data, \n",
    "                         family = binomial)\n",
    "  \n",
    "  # Print model summary\n",
    "  cat(\"\\nAncestry: AMPPD\\n\")\n",
    "  print(summary(glm_interaction))\n",
    "  \n",
    "  # Print coefficients with their names\n",
    "  cat(\"\\nCoefficients and names:\\n\")\n",
    "  coeffs <- coef(glm_interaction)\n",
    "  for (name in names(coeffs)) {\n",
    "    cat(name, \": \", coeffs[name], \"\\n\")\n",
    "  }\n",
    "  \n",
    "  # Print odds ratios with 95% confidence intervals\n",
    "  cat(\"\\nOdds Ratios and 95% CI:\\n\")\n",
    "  odds_ratios <- exp(cbind(coef(glm_interaction), suppressMessages(confint(glm_interaction))))\n",
    "  print(odds_ratios)\n",
    "  \n",
    "  cat(\"\\n\", strrep(\"=\", 80), \"\\n\")\n",
    "  \n",
    "} else {\n",
    "  cat(\"\\nFile not found for AMPPD\\n\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files to the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the final results to workspace bucket (move from VM to workspace bucket)\n",
    "\n",
    "# Burden\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.burden.* {WORKSPACE_BUCKET}/PGLYRP2.burden/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Save the final results to workspace bucket (move from VM to workspace bucket)\n",
    "\n",
    "WORK_DIR = f'/home/jupyter/PGLYRP2_AMPPD'\n",
    "\n",
    "# GLM results - adjusted\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid.adjusted {WORKSPACE_BUCKET}/PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid.adjusted')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid.adjusted {WORKSPACE_BUCKET}/PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid.adjusted')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid.adjusted {WORKSPACE_BUCKET}/PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid.adjusted')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid.adjusted {WORKSPACE_BUCKET}/PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid.adjusted')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid.adjusted {WORKSPACE_BUCKET}/PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid.adjusted')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid.adjusted {WORKSPACE_BUCKET}/PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid.adjusted')\n",
    "\n",
    "# GLM results - glm\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age.txt {WORKSPACE_BUCKET}/PGLYRP2.AMPPD.allvariants_glm_age.txt')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage.txt {WORKSPACE_BUCKET}/PGLYRP2.AMPPD.allvariants_glm_noage.txt')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age_males.txt {WORKSPACE_BUCKET}/PGLYRP2.AMPPD.allvariants_glm_age_males.txt')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_age_females.txt {WORKSPACE_BUCKET}/PGLYRP2.AMPPD.allvariants_glm_age_females.txt')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage_males.txt {WORKSPACE_BUCKET}/PGLYRP2.AMPPD.allvariants_glm_noage_males.txt')\n",
    "shell_do(f'gsutil -u {BILLING_PROJECT_ID} cp -r {WORK_DIR}/PGLYRP2.AMPPD.allvariants_glm_noage_females.txt {WORKSPACE_BUCKET}/PGLYRP2.AMPPD.allvariants_glm_noage_females.txt')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/r-cpu:m126"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "323px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
