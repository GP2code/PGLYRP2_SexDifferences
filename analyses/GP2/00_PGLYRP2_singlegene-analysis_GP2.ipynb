{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGLYRP2 - Single Gene Analysis in GP2 genotyping imputed data\n",
    "\n",
    "#### GP2 Data Release 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project: Sex Differences in PGLYRP2 Variant rs892145 in Parkinson's Disease\n",
    "\n",
    "Version: Python/3.10.12\n",
    "\n",
    "Last Updated: 12-JUNE-2025\n",
    "\n",
    "Update Description: Updated gene coordinates and issues with removal of related individuals\n",
    "\n",
    "Gene coordinates PGLYRP2 from NCBI gene: hg38 (chr19:15468645-15479501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook overview\n",
    "In this notebook we performed regression analyses with PGLYRP2 gene variants and PD in the GP2 NBA data using PLINK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Description\n",
    "\n",
    "* Loading Python libraries\n",
    "* Set paths to the GP2 data\n",
    "* Install packages\n",
    "* Create a covariate file with GP2 data\n",
    "* Annotation of the gene\n",
    "* Turn binary files into VCF\n",
    "* Annotate the gene using ANNOVAR\n",
    "* Burden analyses using RVTESTs\n",
    "* Case/control analyses\n",
    "* Sex stratified case/control analyses\n",
    "* Interaction analyses\n",
    "* Copy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the os package to interact with the environment\n",
    "import os\n",
    "\n",
    "# Bring in Pandas for Dataframe functionality\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Numpy for basics\n",
    "import numpy as np\n",
    "\n",
    "# Use pathlib for file path manipulation\n",
    "import pathlib\n",
    "\n",
    "# Use StringIO for working with file contents\n",
    "from io import StringIO\n",
    "\n",
    "# Enable IPython to display matplotlib graphs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the iPython HTML rendering for displaying links to Google Cloud Console\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Import urllib modules for building URLs to Google Cloud Console\n",
    "import urllib.parse\n",
    "\n",
    "# BigQuery for querying data\n",
    "from google.cloud import bigquery\n",
    "\n",
    "#Import Sys\n",
    "import sys as sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set path to GP2_RELEASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Set path to:\n",
    "\n",
    "# EXTENDED_CLINICAL_DATA_PATH\n",
    "# CLINICAL_DATA_PATH \n",
    "# EXTENDED_CLINICAL_DATA_PATH\n",
    "# RELATED_DATA_PATH\n",
    "# RAW_GENO_PATH\n",
    "# IMPUTED_GENO_PATH\n",
    "# PCS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install plink 1.9\n",
    "cd /home/jupyter/\n",
    "if test -e /home/jupyter/plink; then\n",
    "    echo \"Plink is already installed in /home/jupyter/\"\n",
    "else\n",
    "    echo \"Plink is not installed\"\n",
    "    cd /home/jupyter\n",
    "\n",
    "    wget http://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20190304.zip \n",
    "\n",
    "    unzip -o plink_linux_x86_64_20190304.zip\n",
    "    mv plink plink1.9\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# chmod plink 1.9 to make sure you have permission to run the program\n",
    "chmod u+x /home/jupyter/plink1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install plink 2.0\n",
    "cd /home/jupyter/\n",
    "if test -e /home/jupyter/plink2; then\n",
    "\n",
    "echo \"Plink2 is already installed in /home/jupyter/\"\n",
    "else\n",
    "echo \"Plink2 is not installed\"\n",
    "cd /home/jupyter/\n",
    "\n",
    "wget http://s3.amazonaws.com/plink2-assets/plink2_linux_x86_64_latest.zip\n",
    "\n",
    "unzip -o plink2_linux_x86_64_latest.zip\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# chmod plink 2 to make sure you have permission to run the program\n",
    "chmod u+x /home/jupyter/plink2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install ANNOVAR: We are adding the download link after registration on the annovar website\n",
    "# https://www.openbioinformatics.org/annovar/annovar_download_form.php\n",
    "\n",
    "if test -e /home/jupyter/annovar; then\n",
    "\n",
    "echo \"annovar is already installed in /home/jupyter/notebooks\"\n",
    "else\n",
    "echo \"annovar is not installed\"\n",
    "cd /home/jupyter/\n",
    "\n",
    "wget http://www.openbioinformatics.org/annovar/download/0wgxR2rIVP/annovar.latest.tar.gz\n",
    "\n",
    "tar xvfz annovar.latest.tar.gz\n",
    "\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install BCFtools\n",
    "\n",
    "if test -e /home/jupyter/bcftools; then\n",
    "    echo \"BCFtools is already installed in /home/jupyter/bcftools\"\n",
    "else\n",
    "    echo \"BCFtools is not installed\"\n",
    "    cd /home/jupyter/\n",
    "\n",
    "    # Download the latest version of BCFtools\n",
    "    wget https://github.com/samtools/bcftools/releases/download/1.21/bcftools-1.21.tar.bz2\n",
    "\n",
    "    # Extract the downloaded file\n",
    "    tar -xvjf bcftools-1.21.tar.bz2\n",
    "\n",
    "    # Move into the extracted directory\n",
    "    cd bcftools-1.21\n",
    "\n",
    "    # Compile and install BCFtools\n",
    "    make\n",
    "    make install\n",
    "\n",
    "    # Move the installed BCFtools to a specific directory\n",
    "    mv bcftools /home/jupyter/bcftools\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "\n",
    "# Install ANNOVAR: Download resources for annotation\n",
    "\n",
    "cd /home/jupyter/annovar/\n",
    "#perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar refGene humandb/\n",
    "perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar clinvar_20170905 humandb/\n",
    "#perl annotate_variation.pl -buildver hg38 -downdb cytoBand humandb/\n",
    "#perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar ensGene humandb/\n",
    "#perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar exac03 humandb/ \n",
    "#perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar avsnp147 humandb/ \n",
    "perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar dbnsfp47a humandb/ #latest version of dbNSFP\n",
    "#perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar gnomad211_genome humandb/\n",
    "#perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar ljb26_all humandb/\n",
    "perl annotate_variation.pl -buildver hg38 -downdb -webfrom annovar refGene humandb/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/annovar/humandb\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Install RVTESTS: Option 1 (~15min)\n",
    "if [ ! -e /home/jupyter/tooles/rvtests ]; then\n",
    "    echo \"RVTESTS not found. Installing...\"\n",
    "    mkdir -p /home/jupyter/tools/rvtests\n",
    "    cd /home/jupyter/tools/rvtests\n",
    "\n",
    "    wget https://github.com/zhanxw/rvtests/releases/download/v2.1.0/rvtests_linux64.tar.gz \n",
    "\n",
    "    tar -zxvf rvtests_linux64.tar.gz\n",
    "else\n",
    "    echo \"RVTESTS is already installed.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/jupyter/tools/rvtests/executable\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! chmod 777 /home/jupyter/tools/rvtests/executable/rvtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 777 /home/jupyter/tools/rvtests/executable/rvtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "/home/jupyter/tools/rvtests/executable/rvtest --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a covariate file with GP2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's load the master key\n",
    "key = pd.read_csv(CLINICAL_DATA_PATH, low_memory=False)\n",
    "print(f'Clinical data (num rows, num columns): {key.shape}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "key.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subsetting to keep only a few columns \n",
    "key = key[['GP2ID', 'baseline_GP2_phenotype_for_qc', 'biological_sex_for_qc', 'age_at_sample_collection', 'age_of_onset', 'nba_label','wgs_label']]\n",
    "# Renaming the columns\n",
    "key.rename(columns = {'GP2ID':'IID',\n",
    "                                     'baseline_GP2_phenotype_for_qc':'phenotype',\n",
    "                                     'biological_sex_for_qc':'SEX', \n",
    "                                     'age_at_sample_collection':'AGE', \n",
    "                                     'age_of_onset':'AAO'}, inplace = True)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "key[\"label\"] = key[\"nba_label\"].combine_first(key[\"wgs_label\"])\n",
    "key = key.drop(columns=[\"nba_label\", \"wgs_label\"])\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/PGLYRP2_NBA_R9/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ancestries = {'AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH'}\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    !mkdir /home/jupyter/PGLYRP2_NBA_R9/{ancestry}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_df = pd.read_csv(f'{RELATED_DATA_PATH}/{ancestry}_release9_vwb.related')\n",
    "related_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ancestries = {'AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH'}\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    ## Subset to keep ancestry of interest \n",
    "    ancestry_key = key[key['label']==ancestry].copy()\n",
    "    ancestry_key.reset_index(drop=True)\n",
    "\n",
    "    # Convert phenotype to binary (1/2)\n",
    "    ## Assign conditions so case=2 and controls=1, and -9 otherwise (matching PLINK convention)\n",
    "    # PD = 2; control = 1\n",
    "    pheno_mapping = {\"PD\": 2, \"Control\": 1}\n",
    "    ancestry_key['PHENO'] = ancestry_key['phenotype'].map(pheno_mapping).astype('Int64')\n",
    "    \n",
    "    # Check value counts of pheno\n",
    "    ancestry_key['PHENO'].value_counts(dropna=False)\n",
    "    \n",
    "    ## Get the PCs\n",
    "    pcs = pd.read_csv(f'{RAW_GENO_PATH}/{ancestry}/{ancestry}_release9_vwb.eigenvec', sep='\\t')\n",
    "    \n",
    "    #Select just first 5 PCs\n",
    "    selected_columns = ['IID', 'PC1', 'PC2', 'PC3', 'PC4', 'PC5']\n",
    "    pcs = pd.DataFrame(data=pcs.iloc[:, 1:7].values, columns=selected_columns)\n",
    "    \n",
    "    # Drop the first row (since it's now the column names)\n",
    "    pcs = pcs.drop(0)\n",
    "    \n",
    "    # Reset the index to remove any potential issues\n",
    "    pcs = pcs.reset_index(drop=True)\n",
    "    \n",
    "    # Check size\n",
    "    print(f'PCs: {pcs.shape}')\n",
    "    \n",
    "    # Check value counts of SEX\n",
    "    sex_og_values = ancestry_key['SEX'].value_counts(dropna=False)\n",
    "    print(f'Sex value counts - original:\\n {sex_og_values.to_string()}')\n",
    "    \n",
    "    # Convert sex to binary (1/2)\n",
    "    ## Assign conditions so female=2 and men=1, and -9 otherwise (matching PLINK convention)\n",
    "    # Female = 2; Male = 1\n",
    "    sex_mapping = {\"Female\": 2, \"Male\": 1}\n",
    "    ancestry_key['SEX'] = ancestry_key['SEX'].map(sex_mapping).astype('Int64')\n",
    "    \n",
    "    # Check value counts of SEX after recoding\n",
    "    sex_recode_values = ancestry_key['SEX'].value_counts(dropna=False)\n",
    "    print(f'Sex value counts - recoded:\\n{sex_recode_values.to_string()}')\n",
    "    \n",
    "    ## Make covariate file\n",
    "    df = pd.merge(ancestry_key,pcs, on='IID')\n",
    "    print(f'Check columns for covariate file: {df.columns}')\n",
    "    \n",
    "    # Load information about related individuals in the ancestry analyzed\n",
    "    related_df = pd.read_csv(f'{RELATED_DATA_PATH}/{ancestry}_release9_vwb.related')\n",
    "    related_df['IID1'] = related_df['IID1'].str.replace('_s1', '', regex=False)\n",
    "    print(f'Related individuals: {related_df.shape}')\n",
    "    \n",
    "    # Make a list of just one set of related people\n",
    "    related_list = list(related_df['IID1'])\n",
    "    print(f'Number of related IIDs in dataset before filtering: {df[\"IID\"].isin(related_list).sum()}')\n",
    "    \n",
    "    # Check value counts of related and remove only one related individual\n",
    "    df = df[~df[\"IID\"].isin(related_list)]\n",
    "    \n",
    "    # Check size\n",
    "    print(f'Unrelated individuals: {df.shape}')\n",
    "    \n",
    "    #Make additional columns - FID, fatid and matid - these are needed for RVtests!!\n",
    "    #RVtests needs the first 5 columns to be fid, iid, fatid, matid and sex otherwise it does not run correctly\n",
    "    #Uppercase column name is ok\n",
    "    #See https://zhanxw.github.io/rvtests/#phenotype-file\n",
    "    df['FID'] = 0\n",
    "    df['FATID'] = 0\n",
    "    df['MATID'] = 0\n",
    "    \n",
    "    ## Clean up and keep columns we need\n",
    "    final_df = df[['FID','IID', 'FATID', 'MATID', 'SEX', 'AGE','AAO', 'PHENO','PC1', 'PC2', 'PC3', 'PC4', 'PC5']].copy()\n",
    "    \n",
    "    ##DO NOT replace missing values with -9 as this is misinterpreted by RVtests - needs to be nonnumeric\n",
    "    #Leave missing values as NA\n",
    "    \n",
    "    #Check number of PD cases missing age\n",
    "    pd_missAge = final_df[(final_df['PHENO']==2)&(final_df['AGE'].isna())]\n",
    "    print(f'Number of PD cases missing age: {pd_missAge.shape[0]}')\n",
    "    \n",
    "    #Check number of controls missing age\n",
    "    control_missAge = final_df[(final_df['PHENO']==1)&(final_df['AGE'].isna())]\n",
    "    print(f'Number of controls missing age: {control_missAge.shape[0]}')\n",
    "   \n",
    "    ## Make file of sample IDs to keep\n",
    "    samples_toKeep = final_df[['FID', 'IID']].copy()\n",
    "    samples_toKeep.columns = ['#FID','IID']\n",
    "    samplestokeep_path = pathlib.Path(pathlib.Path.home(), f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep')\n",
    "    \n",
    "    # Create the output CSV file's parent folder in the cloud storage bucket, if it doesn't already exist.\n",
    "    if not samplestokeep_path.parent.exists():\n",
    "        !mkdir -p {samplestokeep_path.parent}\n",
    "        print(f'Created {samplestokeep_path.parent}')\n",
    "    samples_toKeep.to_csv(samplestokeep_path, sep = '\\t', index=False)\n",
    "    finaldf_path = pathlib.Path(pathlib.Path.home(), f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt')\n",
    "    \n",
    "    # Create the output CSV file's parent folder in the cloud storage bucket, if it doesn't already exist.\n",
    "    if not finaldf_path.parent.exists():\n",
    "        !mkdir -p {finaldf_path.parent}\n",
    "        print(f'Created {finaldf_path.parent}')\n",
    "   \n",
    "    final_df.to_csv(finaldf_path, sep = '\\t', na_rep='NA', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/EUR/EUR_covariate_file.txt', sep='\\t')\n",
    "finaldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplestokeep = pd.read_csv('/home/jupyter/PGLYRP2_NBA_R9/EUR/EUR.samplestokeep', sep='\\t')\n",
    "samplestokeep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check mean age and SD for cases and controls, also divided by the sexes\n",
    "\n",
    "ancestries = {'AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH'}\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    print(f'\\n\\nWORKING ON: {ancestry}')\n",
    "    final_df = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt', sep='\\t')\n",
    "\n",
    "    # Check value counts of PHENO (e.g., 1 = control, 2 = case)\n",
    "    pheno_counts = final_df['PHENO'].value_counts(dropna=False)\n",
    "    print(f'PHENO counts:\\n{pheno_counts}')\n",
    "\n",
    "    # Mean and SD of AGE per PHENO\n",
    "    age_stats = final_df.groupby('PHENO')['AGE'].agg(['mean', 'std']).rename(columns={'mean': 'Mean Age', 'std': 'SD Age'})\n",
    "    print(f'Mean age and SD:\\n{age_stats}')\n",
    "\n",
    "    # Counts of SEX per PHENO\n",
    "    sex_counts = pd.crosstab(final_df['PHENO'], final_df['SEX'])\n",
    "    sex_counts.columns = ['Male (1)', 'Female (2)']  # Assuming SEX: 1=Male, 2=Female\n",
    "    print(f'Sex counts:\\n{sex_counts}')\n",
    "\n",
    "    # Optional: Add sex percentage if you want to see proportions too\n",
    "    sex_pct = pd.crosstab(final_df['PHENO'], final_df['SEX'], normalize='index') * 100\n",
    "    sex_pct.columns = ['% Male', '% Female']\n",
    "    print(f'Sex percentages:\\n{sex_pct}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Males - Check N cases and controls for individuals with age data:\n",
    "\n",
    "ancestries = {'AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH'}\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    print(f'\\n\\nWORKING ON - MALES: {ancestry}')\n",
    "    final_df = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt', sep='\\t')\n",
    "\n",
    "    #Males\n",
    "    df_filtered_males_age = final_df[final_df['AGE'].notna() & (final_df['AGE'] != '') & (final_df['SEX'] == 1)]\n",
    "\n",
    "    # Check value counts of PHENO (e.g., 1 = control, 2 = case)\n",
    "    pheno_counts = df_filtered_males_age['PHENO'].value_counts(dropna=False)\n",
    "    print(f'PHENO counts:\\n{pheno_counts}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Females - Check N cases and controls for individuals with age data:\n",
    "\n",
    "ancestries = {'AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH'}\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    print(f'\\n\\nWORKING ON - FEMALES: {ancestry}')\n",
    "    final_df = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt', sep='\\t')\n",
    "\n",
    "    #Males\n",
    "    df_filtered_males_age = final_df[final_df['AGE'].notna() & (final_df['AGE'] != '') & (final_df['SEX'] == 2)]\n",
    "\n",
    "    # Check value counts of PHENO (e.g., 1 = control, 2 = case)\n",
    "    pheno_counts = df_filtered_males_age['PHENO'].value_counts(dropna=False)\n",
    "    print(f'PHENO counts:\\n{pheno_counts}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Annotation of the gene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract the region using PLINK\n",
    "\n",
    "- Extract *PGLYRP2* gene \n",
    "- *PGLYRP2* coordinates: chr19:15468645-15479501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## extract region using plink\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --pfile {IMPUTED_GENO_PATH}/{ancestry}/chr19_{ancestry}_release9_vwb \\\n",
    "    --chr 19 \\\n",
    "    --from-bp 15468645 \\\n",
    "    --to-bp 15479501 \\\n",
    "    --mac 2 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --make-bed \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize bim file\n",
    "! head /home/jupyter/PGLYRP2_NBA_R9/EUR/EUR_PGLYRP2.bim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize fam file\n",
    "! head /home/jupyter/PGLYRP2_NBA_R9/EUR/EUR_PGLYRP2.fam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn binary files into VCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ancestry in ancestries:\n",
    "        \n",
    "    ## Turn binary files into VCF\n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --recode vcf id-paste=iid \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update -y\n",
    "!sudo apt-get install -y tabix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Bgzip and Tabix (zip and index the file)\n",
    "for ancestry in ancestries:    \n",
    "    ! bgzip -f /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.vcf\n",
    "    ! tabix -f -p vcf /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.vcf.gz \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate using ANNOVAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## annotate using ANNOVAR\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "for ancestry in ancestries:\n",
    "        \n",
    "    ! perl /home/jupyter//annovar/table_annovar.pl /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.vcf.gz /home/jupyter/annovar/humandb/ -buildver hg38 \\\n",
    "    -out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.annovar \\\n",
    "    -remove -protocol refGene,clinvar_20170905,dbnsfp47a \\\n",
    "    -operation g,f,f \\\n",
    "    --nopolish \\\n",
    "    -nastring . \\\n",
    "    -vcfinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/home/jupyter/PGLYRP2_NBA_R9/CAS/CAS_PGLYRP2.annovar.hg38_multianno.txt',sep='\\t')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "        \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    # Read in ANNOVAR multianno file\n",
    "    gene = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.annovar.hg38_multianno.txt', sep = '\\t')\n",
    "    \n",
    "    #Filter for the correct gene name (sometimes other genes are also included)\n",
    "    gene = gene[gene['Gene.refGene'] == 'PGLYRP2']\n",
    "    \n",
    "    # Convert the CADD scores to float, set errors to NaN\n",
    "    gene['CADD_phred'] = pd.to_numeric(gene['CADD_phred'], errors='coerce')\n",
    "\n",
    "    #Print number of variants in the different categories\n",
    "    results = [] \n",
    "\n",
    "    intronic = gene[gene['Func.refGene']== 'intronic']\n",
    "    upstream = gene[gene['Func.refGene']== 'upstream']\n",
    "    downstream = gene[gene['Func.refGene']== 'downstream']\n",
    "    utr5 = gene[gene['Func.refGene']== 'UTR5']\n",
    "    utr3 = gene[gene['Func.refGene']== 'UTR3']\n",
    "    splicing = gene[gene['Func.refGene']== 'splicing']\n",
    "    exonic = gene[gene['Func.refGene']== 'exonic']\n",
    "    stopgain = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'stopgain')]\n",
    "    stoploss = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'stoploss')]\n",
    "    startloss = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'startloss')]\n",
    "    frameshift_deletion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'frameshift deletion')]\n",
    "    frameshift_insertion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'frameshift insertion')]\n",
    "    nonframeshift_deletion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'nonframeshift deletion')]\n",
    "    nonframeshift_insertion = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'nonframeshift insertion')]\n",
    "    coding_nonsynonymous = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'nonsynonymous SNV')]\n",
    "    coding_synonymous = gene[(gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] == 'synonymous SNV')]\n",
    "        \n",
    "    print({ancestry})\n",
    "    print('Total variants: ', len(gene))\n",
    "    print(\"Intronic: \", len(intronic))\n",
    "    print(\"Upstream: \", len(upstream))\n",
    "    print(\"Downstream: \", len(downstream))\n",
    "    print('UTR3: ', len(utr3))\n",
    "    print('UTR5: ', len(utr5))\n",
    "    print(\"Splicing: \", len(splicing))\n",
    "    print(\"Total exonic: \", len(exonic))\n",
    "    print(\"Stopgain: \", len(stopgain))\n",
    "    print(\"Stoploss: \", len(stoploss))\n",
    "    print(\"Startloss: \", len(startloss))\n",
    "    print(\"Frameshift deletion: \", len(frameshift_deletion))\n",
    "    print(\"Frameshift insertion: \", len(frameshift_insertion))\n",
    "    print(\"Non-frameshift insertion: \", len(nonframeshift_insertion))\n",
    "    print(\"Non-frameshift deletion: \", len(nonframeshift_deletion))\n",
    "    print('Synonymous: ', len(coding_synonymous))\n",
    "    print(\"Nonsynonymous: \", len(coding_nonsynonymous))\n",
    "    results.append((gene, intronic, upstream, downstream, utr3, utr5, splicing,exonic,stopgain,stoploss,startloss, frameshift_deletion,frameshift_insertion,nonframeshift_deletion,nonframeshift_insertion,coding_synonymous, coding_nonsynonymous))\n",
    "    print('\\n')\n",
    "    \n",
    "    ## For rvtests\n",
    "    \n",
    "    # Potential functional: These are variants annotated as frameshift, nonframeshift, startloss, stoploss, stopgain, splicing, missense, exonic, UTR5, UTR3, upstream (-100bp), downstream (+100bp), or ncRNA. \n",
    "    potentially_functional = gene[gene['Func.refGene'] != 'intronic']\n",
    "    # Coding: These are variants annotated as frameshift, nonframeshift, startloss, stoploss, stopgain, splicing, or missense.\n",
    "    coding_variants = gene[(gene['Func.refGene'] == 'splicing') | (gene['Func.refGene'] == 'exonic') & (gene['ExonicFunc.refGene'] != 'synonymous SNV')]\n",
    "    # Loss of function: These are variants annotated as frameshift, startloss,stopgain, or splicing.\n",
    "    loss_of_function = gene[(gene['Func.refGene'] == 'splicing') | (gene['ExonicFunc.refGene'] == 'stopgain') | (gene['ExonicFunc.refGene'] == 'startloss') | (gene['ExonicFunc.refGene'] == 'frameshift deletion') | (gene['ExonicFunc.refGene'] == 'frameshift insertion')]\n",
    "    \n",
    "    # Save in PLINK format\n",
    "    variants_toKeep = potentially_functional[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.potentially_functional.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    variants_toKeep2 = coding_variants[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep2.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.coding_variants.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    variants_toKeep3 = loss_of_function[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep3.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.loss_of_function.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "        \n",
    "    maf_01 = gene[gene['Otherinfo1'] < 0.01]\n",
    "    variants_toKeep4 = maf_01[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep4.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.maf_01.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    maf_03 = gene[gene['Otherinfo1'] < 0.03]\n",
    "    variants_toKeep5 = maf_03[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep5.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.maf_03.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    # For assoc\n",
    "    \n",
    "    # These are all exonic variants\n",
    "    exonic = gene[gene['Func.refGene'] == 'exonic']\n",
    "    \n",
    "    # Save in PLINK format\n",
    "    variants_toKeep7 = exonic[['Chr', 'Start', 'End', 'Gene.refGene']].copy()\n",
    "    variants_toKeep7.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.exonic.variantstoKeep.txt', sep=\"\\t\", index=False, header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head /home/jupyter/PGLYRP2_NBA_R9/EAS/EAS_PGLYRP2.rare_CADD.variantstoKeep.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Burden Analyses using RVTests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prepare the file format for RVTESTs\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "variant_classes = ['potentially_functional', 'coding_variants','loss_of_function','maf_01','maf_03']\n",
    "\n",
    "#Loop over all the ancestries and the 3 variant classes\n",
    "for ancestry in ancestries:\n",
    "    for variant_class in variant_classes:\n",
    "                \n",
    "        # Print the command to be executed (for debugging purposes)\n",
    "        print(f'Running plink to extract {variant_class} variants for ancestry: {ancestry}')\n",
    "        \n",
    "        #Extract relevant variants\n",
    "        ! /home/jupyter/plink2 \\\n",
    "        --pfile {IMPUTED_GENO_PATH}/{ancestry}/chr19_{ancestry}_release9_vwb \\\n",
    "        --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "        --extract range /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.{variant_class}.variantstoKeep.txt \\\n",
    "        --recode vcf-iid \\\n",
    "        --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.{variant_class}\n",
    "        \n",
    "        # Print the command to be executed (for debugging purposes)\n",
    "        print(f'Running bgzip and tabix for {variant_class} variants for ancestry: {ancestry}')\n",
    "        \n",
    "        ## Bgzip and Tabix (zip and index the file)\n",
    "        ! bgzip -f /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.{variant_class}.vcf\n",
    "        ! tabix -f -p vcf /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.{variant_class}.vcf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop over the different variant classes \n",
    "#Run with all covariates (including age)\n",
    "#Run RVtests\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "variant_classes = ['potentially_functional', 'coding_variants','loss_of_function','maf_01','maf_03']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    for variant_class in variant_classes:\n",
    "                \n",
    "        # Print the command to be executed (for debugging purposes)\n",
    "        print(f'Running RVtests for {variant_class} variants for ancestry: {ancestry}')\n",
    "        \n",
    "        ## RVtests with covariates \n",
    "        #Make sure the pheno and covariate file starts with the first 5 columsn: fid, iid, fatid, matid, sex\n",
    "        #The pheno-name flag only works when the pheno/covar file is structured properly\n",
    "        ! /home/jupyter/tools/rvtests/executable/rvtest --noweb --hide-covar \\\n",
    "        --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.burden.{variant_class} \\\n",
    "        --kernel skat,skato \\\n",
    "        --inVcf /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.{variant_class}.vcf.gz \\\n",
    "        --pheno /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "        --pheno-name PHENO \\\n",
    "        --gene PGLYRP2 \\\n",
    "        --geneFile /home/jupyter/refFlat.txt \\\n",
    "        --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "        --covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at one of the output file\n",
    "! cat /home/jupyter/PGLYRP2_NBA_R9/EUR/EUR_PGLYRP2.burden.maf_03.Skat.assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop over the different variant classes \n",
    "#Run with all covariates (excluding age)\n",
    "#Run RVtests\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "variant_classes = ['potentially_functional', 'coding_variants','loss_of_function','maf_01','maf_03']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    for variant_class in variant_classes:\n",
    "                \n",
    "        # Print the command to be executed (for debugging purposes)\n",
    "        print(f'Running RVtests for {variant_class} variants for ancestry: {ancestry}')\n",
    "        \n",
    "        ## RVtests with covariates \n",
    "        #Make sure the pheno and covariate file starts with the first 5 columsn: fid, iid, fatid, matid, sex\n",
    "        #The pheno-name flag only works when the pheno/covar file is structured properly\n",
    "        ! /home/jupyter/tools/rvtests/executable/rvtest --noweb --hide-covar \\\n",
    "        --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.burden.{variant_class}_NOAGE \\\n",
    "        --kernel skat,skato \\\n",
    "        --inVcf /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.{variant_class}.vcf.gz \\\n",
    "        --pheno /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "        --pheno-name PHENO \\\n",
    "        --gene PGLYRP2 \\\n",
    "        --geneFile /home/jupyter/refFlat.txt \\\n",
    "        --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "        --covar-name SEX,PC1,PC2,PC3,PC4,PC5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Look at one of the output \n",
    "! cat /home/jupyter/PGLYRP2_NBA_R9/AFR/AFR_PGLYRP2.burden.maf_03_NOAGE.Skat.assoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case/Control Analysis\n",
    "\n",
    "### Glossary\n",
    "\n",
    "- CHR Chromosome code\n",
    "- SNP Variant identifier\n",
    "- A1 Allele 1 (usually minor)\n",
    "- A2 Allele 2 (usually major)\n",
    "- MAF Allele 1 frequency in all subjects\n",
    "- F_A/MAF_A Allele 1 frequency in cases\n",
    "- F_U/MAF_U Allele 1 frequency in controls\n",
    "- NCHROBS_A Number of case allele observations\n",
    "- NCHROBS_U Number of control allele observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL VARIANTS\n",
    "#### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis using plink assoc for ALL variants, not adjusting for any covariates\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "\n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --assoc \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --adjust \\\n",
    "    --allow-no-sex \\\n",
    "    --ci 0.95 \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A).\n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink assoc unadjusted analysis for CODING variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    #Look at assoc results\n",
    "    freq = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants.assoc', delim_whitespace=True)\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    sig_all_nonadj = freq[freq['P']<0.05]\n",
    "    \n",
    "    print(f'There are {len(sig_all_nonadj)} variants with p-value < 0.05')\n",
    "\n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "        \n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['SNP'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "          \n",
    "    #Merge with the assoc file\n",
    "    sig_merge = freq[['SNP','A1','F_A','F_U','A2','L95','OR','U95','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='SNP', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}') \n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_assoc.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - Adjusting for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --glm \\\n",
    "    --adjust \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --ci 0.95 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --covar-name SEX,AGE,PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    #Read in glm results\n",
    "    assoc = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "    assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    significant = assoc_add[assoc_add['P']<0.05]\n",
    "    print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "\n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "    #Merge with the glm file\n",
    "    sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_glm_age.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "        sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True) #Sort on Bonferroni, smallest to largest\n",
    "        print(f\"\\nTop entries for {anc}:\")\n",
    "        print(sorted_glm.head())  # or change to sorted_glm.to_string(index=False) for a cleaner look\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for ancestry: {anc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anc}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - Not adjusting for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --glm \\\n",
    "    --adjust \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --ci 0.95 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --covar-name SEX,PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    #Read in glm results\n",
    "    assoc = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "    assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    significant = assoc_add[assoc_add['P']<0.05]\n",
    "    print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "\n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "    #Merge with the glm file\n",
    "    sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_glm_noage.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "        sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True)\n",
    "        print(f\"\\nTop entries for {anc}:\")\n",
    "        print(sorted_glm.head())  # or change to sorted_glm.to_string(index=False) for a cleaner look\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for ancestry: {anc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anc}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LD calculations for identified variants of interest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking LD of SNPs of interest\n",
    "# Variants that have come up as potentially significant are: \n",
    "#Top entries for AFR (Males - no age):\n",
    "   #CHROM                  ID A1     UNADJ        GC      BONF      HOLM  \\\n",
    "# 0      19  chr19:15470473:T:A  A  0.000665  0.020361  0.043217  0.043217   \n",
    "\n",
    "#LD will be calculated with the rs892145 variant chr19:15475861A>T [GRCh38]\n",
    "\n",
    "#Update the code below with the variants that are of interest\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --ld chr19:15470473:T:A chr19:15475861:A:T \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_ld_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting information on rs892145 for all ancestries + HWE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_assoc.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_glm_age.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"Match in {ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the adjusted results for the SNP\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_glm_noage.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the adjusted results for the SNP\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HWE - rs892145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## extract the SNP and calculate HWE\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --chr 19 \\\n",
    "    --from-bp 15475861 \\\n",
    "    --to-bp 15475861 \\\n",
    "    --keep-if PHENO1==1 \\\n",
    "    --hardy \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2_HWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    file_path = f\"/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2_HWE.hardy\"\n",
    "    print(f\"--- Contents of {file_path} ---\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            print(file.read())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex stratified analyses\n",
    "#### assoc - males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis using plink assoc for ALL variants, not adjusting for any covariates and only in males (=1)\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "\n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --assoc \\\n",
    "    --filter-males \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --adjust \\\n",
    "    --allow-no-sex \\\n",
    "    --ci 0.95 \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_males\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A).\n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --filter-males \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink assoc unadjusted analysis \n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    #Look at assoc results\n",
    "    freq = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_males.assoc', delim_whitespace=True)\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    sig_all_nonadj = freq[freq['P']<0.05]\n",
    "    \n",
    "    print(f'There are {len(sig_all_nonadj)} variants with p-value < 0.05')\n",
    "\n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_males.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "        \n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['SNP'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "          \n",
    "    #Merge with the assoc file\n",
    "    sig_merge = freq[['SNP','A1','F_A','F_U','A2','L95','OR','U95','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='SNP', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}') \n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_assoc_males.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - males - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --glm \\\n",
    "    --filter-males \\\n",
    "    --adjust \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --ci 0.95 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --covar-name AGE,PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_males\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --filter-males \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    #Read in glm results\n",
    "    assoc = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "    assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    significant = assoc_add[assoc_add['P']<0.05]\n",
    "    print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_males.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "\n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "    #Merge with the glm file\n",
    "    sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_age_males.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "        sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True) #Sort on Bonferroni, smallest to largest\n",
    "        print(f\"\\nTop entries for {anc}:\")\n",
    "        print(sorted_glm.head())  # or change to sorted_glm.to_string(index=False) for a cleaner look\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for ancestry: {anc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anc}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - males - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --glm \\\n",
    "    --adjust \\\n",
    "    --filter-males \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --ci 0.95 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --covar-name PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_males\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --filter-males \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    #Read in glm results\n",
    "    assoc = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "    assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    significant = assoc_add[assoc_add['P']<0.05]\n",
    "    print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_males.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "\n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "    #Merge with the glm file\n",
    "    sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_noage_males.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "        sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True) #Sort on Bonferroni, smallest to largest\n",
    "        print(f\"\\nTop entries for {anc}:\")\n",
    "        print(sorted_glm.head())  # or change to sorted_glm.to_string(index=False) for a cleaner look\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for ancestry: {anc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anc}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assoc - females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis using plink assoc for ALL variants, not adjusting for any covariates and only in females (=2)\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "\n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --assoc \\\n",
    "    --filter-females \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --adjust \\\n",
    "    --allow-no-sex \\\n",
    "    --ci 0.95 \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_females\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A).\n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --filter-females \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink assoc unadjusted analysis for CODING variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    #Look at assoc results\n",
    "    freq = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_females.assoc', delim_whitespace=True)\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    sig_all_nonadj = freq[freq['P']<0.05]\n",
    "    \n",
    "    print(f'There are {len(sig_all_nonadj)} variants with p-value < 0.05')\n",
    "\n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_females.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "        \n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['SNP'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "          \n",
    "    #Merge with the assoc file\n",
    "    sig_merge = freq[['SNP','A1','F_A','F_U','A2','L95','OR','U95','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='SNP', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}') \n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_assoc_females.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - females - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --glm \\\n",
    "    --filter-females \\\n",
    "    --adjust \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --ci 0.95 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --covar-name AGE,PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_females\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --filter-females \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    #Read in glm results\n",
    "    assoc = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "    assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    significant = assoc_add[assoc_add['P']<0.05]\n",
    "    print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_females.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "\n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "    #Merge with the glm file\n",
    "    sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_age_females.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "        sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True) #Sort on Bonferroni, smallest to largest\n",
    "        print(f\"\\nTop entries for {anc}:\")\n",
    "        print(sorted_glm.head())  # or change to sorted_glm.to_string(index=False) for a cleaner look\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for ancestry: {anc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anc}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glm - females - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run case-control analysis for all variants with covariates\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --glm \\\n",
    "    --adjust \\\n",
    "    --filter-females \\\n",
    "    --maf 0.01 \\\n",
    "    --mac 2 \\\n",
    "    --ci 0.95 \\\n",
    "    --hwe 0.0001 \\\n",
    "    --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --covar-name PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --covar-variance-standardize \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_females\n",
    "    \n",
    "    #--recode A creates a new text fileset, showing each variant in each case and control for the minor allele (A). \n",
    "    ! /home/jupyter/plink1.9 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --filter-females \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process results from plink glm analysis for ALL variants\n",
    "#As there are very few or no significant variants with p-value < 0.05 - we will save results dataframe of all coding variants\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    \n",
    "    print(f'WORKING ON: {ancestry}')\n",
    "    \n",
    "    #Read in glm results\n",
    "    assoc = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid', delim_whitespace=True)\n",
    "    assoc_add = assoc[assoc['TEST']==\"ADD\"]\n",
    "    \n",
    "    #Filter for significant variants p < 0.05 - if any\n",
    "    significant = assoc_add[assoc_add['P']<0.05]\n",
    "    print(f'There are {len(significant)} variants with p-value < 0.05 in glm')\n",
    "    \n",
    "    #Read in plink recoded data (.raw file)\n",
    "    recode = pd.read_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_females.raw', delim_whitespace=True)\n",
    "\n",
    "    # Make a list from the column names\n",
    "    column_names = recode.columns.tolist()\n",
    "\n",
    "    # Drop the first 6 columns to keep the variants \n",
    "    variants = column_names[6:]\n",
    "\n",
    "    print(f'Number of variants in {ancestry} for PGLYRP2: {len(variants)}')\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    cases_data = recode[recode['PHENOTYPE'] == 2]\n",
    "    controls_data = recode[recode['PHENOTYPE'] == 1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Pre-filter the dataset\n",
    "    total_cases = cases_data.shape[0]\n",
    "    total_controls = controls_data.shape[0]\n",
    "    results = []\n",
    "\n",
    "    for variant in variants:\n",
    "        ## For PD cases\n",
    "        hom_cases = (cases_data[variant] == 2).sum()\n",
    "        het_cases = (cases_data[variant] == 1).sum()\n",
    "        hom_ref_cases = (cases_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_cases = total_cases - (hom_cases + het_cases + hom_ref_cases)  # Missing data count\n",
    "        freq_cases = (2 * hom_cases + het_cases) / (2 * (total_cases - missing_cases))  # Adjust for missing data in denominator\n",
    "\n",
    "        ## For controls\n",
    "        hom_controls = (controls_data[variant] == 2).sum()\n",
    "        het_controls = (controls_data[variant] == 1).sum()\n",
    "        hom_ref_controls = (controls_data[variant] == 0).sum()  # Homozygous reference genotype\n",
    "        missing_controls = total_controls - (hom_controls + het_controls + hom_ref_controls)  # Missing data count\n",
    "        freq_controls = (2 * hom_controls + het_controls) / (2 * (total_controls - missing_controls))  # Adjust for missing data in denominator\n",
    "    \n",
    "        # Append results in dictionary format\n",
    "        results.append({\n",
    "            'Variant': variant,\n",
    "            'Hom Cases': hom_cases,\n",
    "            'Het Cases': het_cases,\n",
    "            'Hom Ref Cases': hom_ref_cases,\n",
    "            'Missing Cases': missing_cases,\n",
    "            'Total Cases': total_cases,\n",
    "            'Carrier Freq in Cases': freq_cases,\n",
    "            'Hom Controls': hom_controls,\n",
    "            'Het Controls': het_controls,\n",
    "            'Hom Ref Controls': hom_ref_controls,\n",
    "            'Missing Controls': missing_controls,\n",
    "            'Total Controls': total_controls,\n",
    "            'Carrier Freq in Controls': freq_controls\n",
    "        })\n",
    "\n",
    "    # Return\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['ID'] = df_results['Variant'].apply(lambda x: x.rsplit('_', 1)[0])\n",
    "\n",
    "    #Print dimensions of the df_results dataframe\n",
    "    print(f'df_results shape: {df_results.shape}')\n",
    "    \n",
    "    #Merge with the glm file\n",
    "    sig_merge = assoc_add[['ID','A1','A1_FREQ','OBS_CT','L95','OR','U95','LOG(OR)_SE','Z_STAT','P']]\n",
    "    merged = pd.merge(df_results, sig_merge, on='ID', how='right')\n",
    "    \n",
    "    #Print dimensions of the merged dataframe (just adding more columns)\n",
    "    print(f'Merged dataframe shape: {merged.shape}')\n",
    "    \n",
    "    ## Save to CSV\n",
    "    merged.to_csv(f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_noage_females.txt', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at if there are any significant variants in the adjusted analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for anc in ancestries:\n",
    "    file_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        glm_age_adjust = pd.read_csv(file_path, delim_whitespace=True)\n",
    "        sorted_glm = glm_age_adjust.sort_values(by='BONF', ascending=True) #Sort on Bonferroni, smallest to largest\n",
    "        print(f\"\\nTop entries for {anc}:\")\n",
    "        print(sorted_glm.head())  # or change to sorted_glm.to_string(index=False) for a cleaner look\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for ancestry: {anc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anc}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information on rs892145 for all ancestries + HWE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### males"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_assoc_males.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"Match in {ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_age_males.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the adjusted results for the SNP\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_noage_males.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the adjusted results for the SNP - noage\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_males.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_assoc_females.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"Match in {ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_age_females.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the adjusted results for the SNP\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_age_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### glm - no age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.allvariants_noage_females.txt'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['Variant'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the adjusted results for the SNP -no age\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "# Variant to search for\n",
    "target_variant = 'chr19:15475861:A:T'\n",
    "\n",
    "# Loop through each ancestry group\n",
    "for ancestry in ancestries:\n",
    "    filename = f'/home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2.allvariants_noage_females.PHENO1.glm.logistic.hybrid.adjusted'\n",
    "    try:\n",
    "        df = pd.read_csv(filename, sep='\\t')\n",
    "        \n",
    "        # Filter the row where Variant column contains the target variant\n",
    "        match = df[df['ID'].str.contains(target_variant, na=False)]\n",
    "        \n",
    "        if not match.empty:\n",
    "            print(f\"{ancestry}:\")\n",
    "            print(match.to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        else:\n",
    "            print(f\"No match found in {ancestry}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at if there is a significant interaction term between the SNP of interest and sex using R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install R \n",
    "\n",
    "!pip install rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rpy2 and activate the R interface\n",
    "\n",
    "import rpy2.robjects as robjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if R is working correctly \n",
    "\n",
    "robjects.r('R.version.string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use R natively in a cell, load the R magic extension\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"chr19:15475861:A:T\" > /home/jupyter/PGLYRP2_NBA_R9/single_snp.txt\n",
    "!echo \"chr19:15475861:A:T T\" > /home/jupyter/PGLYRP2_NBA_R9/reference_allele.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new covariate file where genotypes are coded additively (0,1,2)\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "\n",
    "    ! /home/jupyter/plink2 \\\n",
    "    --bfile /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2 \\\n",
    "    --extract /home/jupyter/PGLYRP2_NBA_R9/single_snp.txt \\\n",
    "    --keep /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}.samplestokeep \\\n",
    "    --pheno /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --pheno-name PHENO \\\n",
    "    --covar /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_covariate_file.txt \\\n",
    "    --covar-name SEX,PC1,PC2,PC3,PC4,PC5 \\\n",
    "    --ref-allele /home/jupyter/PGLYRP2_NBA_R9/reference_allele.txt \\\n",
    "    --recode A \\\n",
    "    --out /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/{ancestry}_PGLYRP2_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head /home/jupyter/PGLYRP2_NBA_R9/EUR/EUR_PGLYRP2_interaction.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head /home/jupyter/PGLYRP2_NBA_R9/EUR/EUR_PGLYRP2_interaction.cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the ANC_PGLYRP2_interaction.raw with the ANC_PGLYRP2_interaction.cov on IID\n",
    "#Drop SEX from one of the files to avoid duplicates\n",
    "\n",
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for anc in ancestries:\n",
    "    cov_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2_interaction.cov'\n",
    "    raw_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2_interaction.raw'\n",
    "\n",
    "    try:\n",
    "        # Load both files\n",
    "        cov_df = pd.read_csv(cov_path, delim_whitespace=True)\n",
    "        raw_df = pd.read_csv(raw_path, delim_whitespace=True)\n",
    "\n",
    "        # Rename '#IID' to 'IID' for merge compatibility\n",
    "        cov_df = cov_df.rename(columns={'#IID': 'IID'})\n",
    "\n",
    "        # Drop 'SEX' from cov file to avoid duplication (optional, since values are the same)\n",
    "        if 'SEX' in cov_df.columns:\n",
    "            cov_df = cov_df.drop(columns=['SEX'])\n",
    "\n",
    "        # Merge\n",
    "        merged_df = pd.merge(raw_df, cov_df, on='IID')\n",
    "\n",
    "        # Save back to original .raw path (space-delimited, no index)\n",
    "        merged_df.to_csv(raw_path, sep=' ', index=False)\n",
    "\n",
    "        print(f\"Saved merged file: {raw_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Missing file for {anc}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {anc}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the ancestry to inspect\n",
    "anc = 'EUR'\n",
    "\n",
    "# Path to the saved merged file\n",
    "merged_path = f'/home/jupyter/PGLYRP2_NBA_R9/{anc}/{anc}_PGLYRP2_interaction.raw'\n",
    "\n",
    "# Read the merged file\n",
    "merged_df = pd.read_csv(merged_path, delim_whitespace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"\\nPreview of merged file for {anc}:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "file_path <- \"/home/jupyter/PGLYRP2_NBA_R9/AAC/AAC_PGLYRP2_interaction.raw\"\n",
    "lines <- readLines(file_path)\n",
    "cat(\"Line 140 content:\\n\", lines[140], \"\\n\")\n",
    "cat(\"Number of elements:\", length(strsplit(lines[140], \"\\\\s+\")[[1]]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "# Vector of ancestries\n",
    "ancestries <- c('AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH')\n",
    "\n",
    "# Loop over each ancestry\n",
    "for (ancestry in ancestries) {\n",
    "  file_path <- paste0(\"/home/jupyter/PGLYRP2_NBA_R9/\", ancestry, \"/\", ancestry, \"_PGLYRP2_interaction.raw\")\n",
    "  \n",
    "  # Check if file exists\n",
    "  if (file.exists(file_path)) {\n",
    "    cat(\"\\nReading:\", file_path, \"\\n\")\n",
    "    \n",
    "    # Try reading the file with more robust options\n",
    "    interaction_data <- tryCatch({\n",
    "      read.table(file_path,\n",
    "                 header = TRUE,\n",
    "                 sep = \"\",           # treat any whitespace as delimiter\n",
    "                 fill = TRUE,        # fill incomplete lines\n",
    "                 comment.char = \"\",  # disable comments\n",
    "                 quote = \"\",         # disable quote parsing\n",
    "                 fileEncoding = \"UTF-8\")\n",
    "    }, error = function(e) {\n",
    "      cat(\"❌ Error reading file for ancestry:\", ancestry, \"\\n\")\n",
    "      cat(\"   ➤ File path:\", file_path, \"\\n\")\n",
    "      cat(\"   ➤ Error message:\", e$message, \"\\n\\n\")\n",
    "      return(NULL)\n",
    "    })\n",
    "    \n",
    "    # Proceed only if successful\n",
    "    if (!is.null(interaction_data)) {\n",
    "      interaction_data$PHENOTYPE <- ifelse(interaction_data$PHENOTYPE == 2, 1, 0)\n",
    "      interaction_data$SEX <- factor(interaction_data$SEX, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n",
    "      interaction_data$SEX <- relevel(interaction_data$SEX, ref = \"Female\")\n",
    "      \n",
    "      cat(\"✅ Successfully processed:\", ancestry, \"\\n\")\n",
    "      print(head(interaction_data))\n",
    "    }\n",
    "  } else {\n",
    "    cat(\"⚠️ File not found for ancestry:\", ancestry, \"\\n\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "ancestries <- c('AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH')\n",
    "\n",
    "for (ancestry in ancestries) {\n",
    "  file_path <- paste0(\"/home/jupyter/PGLYRP2_NBA_R9/\", ancestry, \"/\", ancestry, \"_PGLYRP2_interaction.raw\")\n",
    "  \n",
    "  if (file.exists(file_path)) {\n",
    "    interaction_data <- read.table(file_path, header = TRUE, sep = \"\",fill = TRUE,comment.char = \"\",quote = \"\", fileEncoding = \"UTF-8\")\n",
    "    \n",
    "    interaction_data$PHENOTYPE <- ifelse(interaction_data$PHENOTYPE == 2, 1, 0)\n",
    "    interaction_data$SEX <- factor(interaction_data$SEX, levels = c(1, 2), labels = c(\"Male\", \"Female\"))\n",
    "    interaction_data$SEX <- relevel(interaction_data$SEX, ref = \"Female\")\n",
    "\n",
    "    colnames(interaction_data) <- make.names(colnames(interaction_data))\n",
    "\n",
    "    # Fit logistic regression model with interaction term\n",
    "    glm_interaction <- glm(PHENOTYPE ~ SEX * `chr19.15475861.A.T_T`+PC1+PC2+PC3+PC4+PC5, data = interaction_data, \n",
    "                           family = binomial)\n",
    "    \n",
    "    # Print ancestry and model summary\n",
    "    cat(\"\\nAncestry:\", ancestry, \"\\n\")\n",
    "    print(summary(glm_interaction))\n",
    "    \n",
    "    # Print coefficients with their names\n",
    "    cat(\"\\nCoefficients and names:\\n\")\n",
    "    coeffs <- coef(glm_interaction)\n",
    "    for (name in names(coeffs)) {\n",
    "      cat(name, \": \", coeffs[name], \"\\n\")\n",
    "    }\n",
    "    \n",
    "    # Print odds ratios with 95% confidence intervals\n",
    "    cat(\"\\nOdds Ratios and 95% CI:\\n\")\n",
    "    odds_ratios <- exp(cbind(coef(glm_interaction), suppressMessages(confint(glm_interaction))))\n",
    "    print(odds_ratios)\n",
    "    \n",
    "    cat(\"\\n\", strrep(\"=\", 80), \"\\n\")\n",
    "    \n",
    "  } else {\n",
    "    cat(\"\\nFile not found for ancestry:\", ancestry, \"\\n\")\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "names(coef(glm_interaction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ancestries = ['AAC', 'AFR', 'AJ', 'AMR', 'CAS', 'EAS', 'EUR', 'FIN', 'MDE', 'SAS', 'CAH']\n",
    "\n",
    "for ancestry in ancestries:\n",
    "    !mkdir /home/jupyter/workspace/pglyrp2/pglyrp2/{ancestry}\n",
    "    !cp /home/jupyter/PGLYRP2_NBA_R9/{ancestry}/* /home/jupyter/workspace/pglyrp2/pglyrp2/{ancestry}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/r-cpu:m126"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "322.972px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
